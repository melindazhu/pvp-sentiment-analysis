{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vf-lqCfM3V1"
      },
      "source": [
        "# Naive Bayes with Not-Predetermined Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTnP71OAOxuG"
      },
      "source": [
        "Reference code: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JOMauYJNcvA"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEPuuOA8Mwrz"
      },
      "source": [
        "Imports and loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuNtnjnsYFRQ"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2IpeubkbAGC"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOFPwaLsYStj"
      },
      "source": [
        "df = pd.read_csv('train1_full.csv') \n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzrcBXLrZL0Z"
      },
      "source": [
        "# Here we get transform the documents into sentences\n",
        "def preprocess(df):\n",
        "    df['comment_text'] = df.comment_text.str.lower()\n",
        "    df['document_sentences'] = df.comment_text.str.split('.') \n",
        "    df['tokenized_sentences'] = list(map(lambda sentences: list(map(nltk.word_tokenize, sentences)), df.document_sentences))  \n",
        "    df['tokenized_sentences'] = list(map(lambda sentences: list(filter(lambda lst: lst, sentences)), df.tokenized_sentences))\n",
        "\n",
        "preprocess(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-RzvtoSaI0c"
      },
      "source": [
        "Split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlG5SC_EZWU6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test, y_train, y_test = train_test_split(df.drop(columns='label'), df['label'], test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9cIgWYjlyvO"
      },
      "source": [
        "def remove_items(test_list, item):\n",
        "    # utility function to remove stop words\n",
        "    for i in test_list:\n",
        "        if(i == item):\n",
        "            test_list.remove(i)\n",
        "  \n",
        "    return test_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_ovXsHol0ez"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    sentence = sen.lower()\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    sentence_list = sentence.split()\n",
        "\n",
        "    # Removing stop words\n",
        "    stop_words = ['u', 'ur', 'im', 'can', 'cant', 'i', 'me', 'my', 'myself', 'we', 'go', 'our', 'ours', 'ourselves', 'you', \"youre\", \"youve\", \"youll\", \"youd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"thatll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"shouldve\", 'now', 'd', 'll', 'm', 'o', 're', 'r', 'ur', 've', 'y', 'ain', 'aren', \"arent\", 'couldn', \"couldnt\", 'didn', \"didnt\", 'doesn', \"doesnt\", 'hadn', \"hadnt\", 'hasn', \"hasnt\", 'haven', \"havent\", 'isn', \"isnt\", 'ma', 'mightn', \"mightnt\", 'mustn', \"mustnt\", 'needn', \"neednt\", 'shan', \"shant\", 'shouldn', \"shouldnt\", 'wasn', \"wasnt\", 'weren', \"werent\", 'won', \"wont\", 'wouldn', \"wouldnt\"]\n",
        "    for stop_word in stop_words:\n",
        "      if stop_word in sentence_list:\n",
        "        sentence_list = remove_items(sentence_list, stop_word)\n",
        "\n",
        "    # Join back to list\n",
        "    sentence = \" \".join(sentence_list)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    return sentence.lstrip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcq-m6qHNpxn"
      },
      "source": [
        "Get a list of all messages and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1NSdlRdl3UK"
      },
      "source": [
        "X_train = []\n",
        "sentences = list(train[\"comment_text\"])\n",
        "for sen in sentences:\n",
        "    X_train.append(preprocess_text(sen))\n",
        "\n",
        "y_train = np.asarray(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UecOcQczmm3K"
      },
      "source": [
        "X_test = []\n",
        "sentences1 = list(test[\"comment_text\"])\n",
        "for sen in sentences1:\n",
        "    X_test.append(preprocess_text(sen))\n",
        "  \n",
        "y_test = np.asarray(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkAMpCb2wNc3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_matrix = vectorizer.fit_transform(X_train)\n",
        "X_test_matrix = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUt9xGAUwX6n"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "NBClf = MultinomialNB()\n",
        "NBClf.fit(X_train_matrix, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlu_xMr5wp-S"
      },
      "source": [
        "predictions = NBClf.predict(X_test_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnJYysOuw8Pq"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGgzayZ0xG-m"
      },
      "source": [
        "print(confusion_matrix(y_test,predictions,normalize='true'))\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}