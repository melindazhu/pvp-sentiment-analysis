{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes Baseline with Vocab.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vf-lqCfM3V1"
      },
      "source": [
        "# Naive Bayes with Not-Predetermined Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTnP71OAOxuG"
      },
      "source": [
        "Sources: CS229 Problem Set 2 spam.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JOMauYJNcvA"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEPuuOA8Mwrz"
      },
      "source": [
        "Imports and loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuNtnjnsYFRQ"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2IpeubkbAGC"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOFPwaLsYStj"
      },
      "source": [
        "df = pd.read_csv('train1_full.csv') \n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzrcBXLrZL0Z"
      },
      "source": [
        "# Here we get transform the documents into sentences\n",
        "def preprocess(df):\n",
        "    df['comment_text'] = df.comment_text.str.lower()\n",
        "    df['document_sentences'] = df.comment_text.str.split('.') \n",
        "    df['tokenized_sentences'] = list(map(lambda sentences: list(map(nltk.word_tokenize, sentences)), df.document_sentences))  \n",
        "    df['tokenized_sentences'] = list(map(lambda sentences: list(filter(lambda lst: lst, sentences)), df.tokenized_sentences))\n",
        "\n",
        "preprocess(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-RzvtoSaI0c"
      },
      "source": [
        "Split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlG5SC_EZWU6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test, y_train, y_test = train_test_split(df.drop(columns='label'), df['label'], test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9cIgWYjlyvO"
      },
      "source": [
        "def remove_items(test_list, item):\n",
        "    # utility function to remove stop words\n",
        "    for i in test_list:\n",
        "        if(i == item):\n",
        "            test_list.remove(i)\n",
        "  \n",
        "    return test_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_ovXsHol0ez"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    sentence = sen.lower()\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    sentence_list = sentence.split()\n",
        "\n",
        "    # Removing stop words\n",
        "    stop_words = ['u', 'ur', 'im', 'can', 'cant', 'i', 'me', 'my', 'myself', 'we', 'go', 'our', 'ours', 'ourselves', 'you', \"youre\", \"youve\", \"youll\", \"youd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"thatll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"shouldve\", 'now', 'd', 'll', 'm', 'o', 're', 'r', 'ur', 've', 'y', 'ain', 'aren', \"arent\", 'couldn', \"couldnt\", 'didn', \"didnt\", 'doesn', \"doesnt\", 'hadn', \"hadnt\", 'hasn', \"hasnt\", 'haven', \"havent\", 'isn', \"isnt\", 'ma', 'mightn', \"mightnt\", 'mustn', \"mustnt\", 'needn', \"neednt\", 'shan', \"shant\", 'shouldn', \"shouldnt\", 'wasn', \"wasnt\", 'weren', \"werent\", 'won', \"wont\", 'wouldn', \"wouldnt\"]\n",
        "    for stop_word in stop_words:\n",
        "      if stop_word in sentence_list:\n",
        "        sentence_list = remove_items(sentence_list, stop_word)\n",
        "\n",
        "    # Join back to list\n",
        "    sentence = \" \".join(sentence_list)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    return sentence.lstrip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcq-m6qHNpxn"
      },
      "source": [
        "Get a list of all messages and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1NSdlRdl3UK"
      },
      "source": [
        "X_train = []\n",
        "sentences = list(train[\"comment_text\"])\n",
        "for sen in sentences:\n",
        "    X_train.append(preprocess_text(sen))\n",
        "\n",
        "y_train = np.asarray(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UecOcQczmm3K"
      },
      "source": [
        "X_test = []\n",
        "sentences1 = list(test[\"comment_text\"])\n",
        "for sen in sentences1:\n",
        "    X_test.append(preprocess_text(sen))\n",
        "  \n",
        "y_test = np.asarray(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yjlic3xVbHu"
      },
      "source": [
        "toxic_phrases = {'unskilled': 0, 'useless': 1, 'fucking': 2, 'motherfucking': 3, 'noob': 4, 'slut': 5, 'fuck': 6, 'fked': 7, 'fucked': 8, 'miserable': 9, 'sobad': 10, 'youaresobad': 11, \n",
        "                 'doesnothing': 12, 'somad': 13, 'scumbag': 14, 'fu': 15, 'youlost': 16, 'youdidthis': 17, 'goplay': 18, 'barbie': 19, 'screw': 20, 'usuck': 21, 'yousuck': 22,\n",
        "                 'shit': 23, 'wakeup': 24, 'uninstall': 25, 'justuninstall': 26, 'retard': 27, 'tard': 28, 'yourmom': 29, 'urmom': 30, 'drugs': 31, 'justleave': 32, 'muted': 33, \n",
        "                 'shitted': 34, 'gay': 35, 'pussy': 36, 'surrend': 37, 'english': 38, 'stfu': 39, 'playyourself': 40, 'ignore': 41, 'twat': 42, 'wrong': 43, 'bronze': 44, 'nooob': 45, \n",
        "                 'suck': 46, 'fuuck': 47, 'worstteam': 48, 'udone': 49, 'uarebad': 50, 'youarebad': 51, 'ubelongin': 52, 'youbelongin': 53, 'shutup': 54, 'fail': 55, 'fuckyou': 56, \n",
        "                 'fucku': 57, 'crap': 58, 'fucker': 59, 'ihate': 60, 'stupid': 61, 'mother': 62, 'gohome': 63, 'youblind': 64, 'kys': 65, 'lowelo': 66, 'idiot': 67, 'trash': 68, \n",
        "                 'fag': 69, 'fking': 70, 'suckhard': 71, 'uneedhelp': 72, 'youneedhelp': 73, 'didnothing': 74, 'hateu': 75, 'hateyou': 76, 'shame': 77, 'bitch': 78, 'worst': 79, \n",
        "                 'biggestnoob': 80, 'pugface': 81, 'fuking': 82, 'udumb': 83, 'fkin': 84, 'die': 85, 'uninstallpls': 86, 'dick': 87, 'suck': 88, 'brainless': 89, 'neverplay': 90, \n",
        "                 'fuckoff': 91, 'nigger': 92, 'yourdad': 93, 'cock': 94, 'godie': 95, 'aids': 96, 'african': 97, 'fat': 98, 'dumbass': 99, 'pile': 100, 'poop': 101, 'nigga': 102, \n",
        "                 'poo': 103, 'ass': 104, 'nub': 105, 'whatanoob': 106, 'goingafk': 107, 'ufeeder': 108, 'stoptalkin': 109, 'braindead': 110, 'wrongwithu': 111, 'justbad': 112, \n",
        "                 'youtried': 113, 'pathetic': 114, 'awful': 115, 'fcking': 116, 'fck': 117, 'cantplay': 118, 'moron': 119, 'goafk': 120, 'shutit': 121, 'justmute': 122, 'whore': 123, \n",
        "                 'muteher': 124, 'mutehim': 125, 'feeder': 126, 'yourfault': 127, 'betterthanyou': 128, 'piss': 129, 'fuckingnoob': 130, 'yourmother': 131, 'delete': 132, 'suckhard': 133, \n",
        "                 'noskill': 134, 'girl': 135, 'lowlife': 136, 'dontlikeyou': 137, 'dontlikeu': 138, 'pieceofshit': 139, 'badplayer': 140, 'fucktard': 141, 'getlost': 142, 'getcarried': 143, \n",
        "                 'gtfo': 144, 'mental': 145, 'youdont': 146, 'sucksohard': 147, 'youcanmute': 148, 'shutthefuckup': 149, 'puss': 150, 'liar': 151, 'faggot': 152, 'shesbad': 153, 'sucking': 154, \n",
        "                 'lowlevel': 155, 'bequiet': 156, 'asshole': 157, 'cry': 158, 'prostitute': 159, 'german': 160, 'youfeeder': 161, 'asianbitch': 162, 'juststop': 163, 'fuman': 164, 'tetris': 165}\n",
        "toxic_phrase_list = []\n",
        "for word in toxic_phrases:\n",
        "  toxic_phrase_list.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS7C387HVb7C"
      },
      "source": [
        "def process_message(message):\n",
        "  \"\"\"\n",
        "  Normalize a message by removing spaces and converting to lowercase.\n",
        "  \"\"\"\n",
        "  no_spaces = message.replace(\" \", \"\")\n",
        "  return no_spaces.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Na-3pJYVnTd"
      },
      "source": [
        "def transform_text(messages, phrase_list):\n",
        "    \"\"\"\n",
        "    This function should create a numpy array that contains the number of times each phrase\n",
        "    of the dictionary appears in each message. \n",
        "    Each row in the resulting array should correspond to each message \n",
        "    and each column should correspond to a word of the vocabulary.\n",
        "\n",
        "    Use the provided word dictionary to map words to column indices. Ignore words that\n",
        "    are not present in the dictionary. Use process_message on each message.\n",
        "\n",
        "    Args:\n",
        "        messages: A list of strings where each string is a chat message.\n",
        "        word_dictionary: A python dict mapping phrases to integers.\n",
        "\n",
        "    Returns:\n",
        "        A numpy array marking the words present in each message.\n",
        "        Where the component (i,j) is the number of occurrences of the\n",
        "        j-th vocabulary phrase in the i-th message.\n",
        "    \"\"\"\n",
        "    arr = np.zeros((len(messages), len(phrase_list)))\n",
        "    for msg_idx in range(len(messages)):\n",
        "        processed = process_message(messages[msg_idx])\n",
        "        for phrase in phrase_list:\n",
        "          if phrase in processed:\n",
        "            count_occurr = processed.count(phrase)\n",
        "            arr[msg_idx, phrase_list.index(phrase)] += (count_occurr)\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlIsmLzgVqTX"
      },
      "source": [
        "train_matrix = transform_text(X_train, toxic_phrase_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc-tQ7zNVuTN"
      },
      "source": [
        "def fit_naive_bayes_model(matrix, labels):\n",
        "    \"\"\"\n",
        "    Return the model parameters (likelihood of each word given class label,\n",
        "    prior probabilities)\n",
        "\n",
        "    Args:\n",
        "        matrix: A numpy array containing word counts for the training data\n",
        "        labels: A vector with labels for the training data\n",
        "\n",
        "    Returns: The trained model parameters\n",
        "    \"\"\"\n",
        "    V = matrix.shape[1] # number of words in dictionary\n",
        "    N = matrix.shape[0] # number of total messages\n",
        "    neutral_messages = matrix[labels == 0]\n",
        "    toxic_messages = matrix[labels == 1]\n",
        "    derog_messages = matrix[labels == 2]\n",
        "\n",
        "    # find likelihood parameters: probabilities of each word given the class label\n",
        "    likelihoods_class0 = np.zeros((V,))\n",
        "    likelihoods_class1 = np.zeros((V,))\n",
        "    likelihoods_class2 = np.zeros((V,))\n",
        "    priors = []\n",
        "\n",
        "    n_neutral_words = np.sum(neutral_messages)\n",
        "    n_toxic_words = np.sum(toxic_messages)\n",
        "    n_derog_words = np.sum(derog_messages)\n",
        "    for w in range(V):\n",
        "        likelihoods_class0[w] = (sum(neutral_messages[:,w]) + 1)/ (n_neutral_words + V)\n",
        "        likelihoods_class1[w] = (sum(toxic_messages[:,w]) + 1)/ (n_toxic_words + V)\n",
        "        likelihoods_class2[w] = (sum(derog_messages[:,w]) + 1)/ (n_derog_words + V)\n",
        "\n",
        "    priors.append(neutral_messages.shape[0] / N)\n",
        "    priors.append(toxic_messages.shape[0] / N)\n",
        "    priors.append(derog_messages.shape[0] / N)\n",
        "\n",
        "    return [likelihoods_class0, likelihoods_class1, likelihoods_class2, priors]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeiKX5VsVzge"
      },
      "source": [
        "def predict_from_naive_bayes_model(messages, model, matrix):\n",
        "    \"\"\"Use a Naive Bayes model to compute predictions for a target matrix.\n",
        "\n",
        "    This function should be able to predict on the models that fit_naive_bayes_model\n",
        "    outputs.\n",
        "\n",
        "    Args:\n",
        "        model: A trained model from fit_naive_bayes_model\n",
        "        matrix: A numpy array containing word counts\n",
        "\n",
        "    Returns: A numpy array containing the predictions from the model (int array of 0 or 1 values)\n",
        "    \"\"\"\n",
        "    likelihoods_class0 = model[0]\n",
        "    likelihoods_class1 = model[1]\n",
        "    likelihoods_class2 = model[2]\n",
        "    priors = model[3]\n",
        "    V = matrix.shape[1]\n",
        "    N = matrix.shape[0]\n",
        "    log_prior0 = math.log(priors[0])\n",
        "    log_prior1 = math.log(priors[1])\n",
        "    log_prior2 = math.log(priors[2])\n",
        "    preds = np.zeros((N,))\n",
        "    for i in range(N):\n",
        "        sum_class0 = 0\n",
        "        sum_class1 = 0\n",
        "        sum_class2 = 0\n",
        "        for t in range(V):\n",
        "            sum_class0 += matrix[i, t] * math.log(likelihoods_class0[t])\n",
        "            sum_class1 += matrix[i, t] * math.log(likelihoods_class1[t])\n",
        "            sum_class2 += matrix[i, t] * math.log(likelihoods_class2[t])\n",
        "        prob_class0 = log_prior0 + sum_class0\n",
        "        prob_class1 = log_prior1 + sum_class1\n",
        "        prob_class2 = log_prior2 + sum_class2\n",
        "        probs = [prob_class0, prob_class1, prob_class2]\n",
        "        class_pred = probs.index(max(probs))\n",
        "        preds[i] = class_pred\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhZFph1sV2Hj"
      },
      "source": [
        "# run on processed matrix\n",
        "params = fit_naive_bayes_model(train_matrix, y_train)\n",
        "# save predicted outputs to a file\n",
        "test_matrix = transform_text(X_test, toxic_phrase_list)\n",
        "preds = predict_from_naive_bayes_model(X_test, params, test_matrix)\n",
        "np.savetxt('predictions.txt', preds)\n",
        "naive_bayes_accuracy = np.mean(preds == y_test)\n",
        "print(naive_bayes_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x44LPTSWGNk"
      },
      "source": [
        "print(confusion_matrix(y_test,preds,normalize='true'))\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,preds))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}