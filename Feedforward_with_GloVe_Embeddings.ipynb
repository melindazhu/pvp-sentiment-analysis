{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feedforward with GloVe Embeddings.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOJJITkCL_Fj"
      },
      "source": [
        "# Feedforward with GloVe Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4_X7YMMEem"
      },
      "source": [
        "GloVe embeddings reference: https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/#disqus_thread\n",
        "\n",
        "Reference code: Deep Learning with Python (Chollet) see report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx4ZkxwyMKkK"
      },
      "source": [
        "# Imports and Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV8Zm_n0MZ3I"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgVvERNAMjrM"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_6yn-9RMawI"
      },
      "source": [
        "comments = pd.read_csv(\"train1_onehot_full.csv\")\n",
        "print(comments.shape)\n",
        "comments.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbtZSnXXGKSs"
      },
      "source": [
        "Remove all rows that contain a null value or empty string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRdj-CT1F-_F"
      },
      "source": [
        "filter = comments[\"comment_text\"] != \"\"\n",
        "comments = comments[filter]\n",
        "comments = comments.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYyiJZO4GdRH"
      },
      "source": [
        "Print a random comment, and print its associated labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kfPU53mGWoH",
        "outputId": "fb34c3bc-572c-4d9a-e49b-1c0389f3cafa"
      },
      "source": [
        "print(comments[\"comment_text\"][160])\n",
        "print(\"Neutral:\" + str(comments[\"neutral\"][160]))\n",
        "print(\"Toxic:\" + str(comments[\"toxic\"][160]))\n",
        "print(\"Derogatory:\" + str(comments[\"derogatory\"][160]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "really terrible\n",
            "Neutral:0\n",
            "Toxic:1\n",
            "Derogatory:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9k-n8-NHmaP"
      },
      "source": [
        "Plot the comment count for each label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lVUHYgbGaKh"
      },
      "source": [
        "comments_labels = comments[[\"neutral\", \"toxic\", \"derogatory\"]]\n",
        "comments_labels.head()\n",
        "print(comments_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umhu5F9wHwzf"
      },
      "source": [
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 8\n",
        "fig_size[1] = 6\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "plt.title('Dataset balances')\n",
        "\n",
        "print(comments_labels.sum(axis=0) / 38640)\n",
        "labels_to_plot = [(comments_labels.sum(axis=0))[0], (comments_labels.sum(axis=0))[1], (comments_labels.sum(axis=0))[2]]\n",
        "plt.bar(['neutral/positive', 'neg. attitude', 'derogatory'], labels_to_plot, width=0.35, align='center')\n",
        "plt.ylim(0, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw55NN6gMsnw"
      },
      "source": [
        "def remove_items(test_list, item):\n",
        "    # remove the item for all its occurrences\n",
        "    for i in test_list:\n",
        "        if(i == item):\n",
        "            test_list.remove(i)\n",
        "  \n",
        "    return test_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTf4963T8paq"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    sentence = sen.lower()\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    sentence_list = sentence.split()\n",
        "\n",
        "    # Removing stop words\n",
        "    stop_words = ['u', 'ur', 'im', 'can', 'cant', 'i', 'me', 'my', 'myself', 'we', 'go', 'our', 'ours', 'ourselves', 'you', \"youre\", \"youve\", \"youll\", \"youd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"thatll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"shouldve\", 'now', 'd', 'll', 'm', 'o', 're', 'r', 'ur', 've', 'y', 'ain', 'aren', \"arent\", 'couldn', \"couldnt\", 'didn', \"didnt\", 'doesn', \"doesnt\", 'hadn', \"hadnt\", 'hasn', \"hasnt\", 'haven', \"havent\", 'isn', \"isnt\", 'ma', 'mightn', \"mightnt\", 'mustn', \"mustnt\", 'needn', \"neednt\", 'shan', \"shant\", 'shouldn', \"shouldnt\", 'wasn', \"wasnt\", 'weren', \"werent\", 'won', \"wont\", 'wouldn', \"wouldnt\"]\n",
        "    for stop_word in stop_words:\n",
        "      if stop_word in sentence_list:\n",
        "        sentence_list = remove_items(sentence_list, stop_word)\n",
        "\n",
        "    # Join back to list\n",
        "    sentence = \" \".join(sentence_list)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    return sentence.lstrip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkfeyditP2Lu"
      },
      "source": [
        "Get a list of all sentences and labels from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlx1JTmd8usJ"
      },
      "source": [
        "X = []\n",
        "sentences = list(comments[\"comment_text\"])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "\n",
        "y = comments_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PObnL5DhQEXF"
      },
      "source": [
        "Split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjqQ5aO_wjy5"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDVa_qkBttZ1"
      },
      "source": [
        "# Tokenizing and GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NErXiyWSYvCQ"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "maxlen = 10\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ZB562LQMHg"
      },
      "source": [
        "Get GloVe embeddings. The embedding matrix will be fed into the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQVQ3Ld3ZFeu"
      },
      "source": [
        "embeddings_dictionary = dict()\n",
        "glove_file = open('glove.6B.50d.txt', encoding=\"utf8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoAWqscZGyM"
      },
      "source": [
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary[word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-XUh9u-fRnp"
      },
      "source": [
        "embedding_matrix = zeros((vocab_size, 50))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT2Llcv1rQUH"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(100, activation='relu', input_shape=(50,)))\n",
        "model.add(layers.Dense(50, activation='sigmoid'))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfcIwldar2Y4"
      },
      "source": [
        "print(embedding_matrix.shape)\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-suFpyRGrmbL"
      },
      "source": [
        "history = model.fit(embedding_matrix, y_train, batch_size=30, epochs=550, verbose=1, validation_split=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJvwZUahx4D5"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}