{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx77sAO79SXk"
      },
      "source": [
        "# Naive Bayes Baseline with Predetermined Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VbSnKUB9cFW"
      },
      "source": [
        "Reference code: CS229 Problem Set 2 spam.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jcLLbngG6--"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEypBhwvE1n7"
      },
      "source": [
        "# Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj8V9mBTb2HQ"
      },
      "source": [
        "league_train1_full = pd.read_csv(\"league_train1.csv\", skiprows=[1], usecols = ['league_train1','Unnamed: 2'])\n",
        "messages1 = league_train1_full['league_train1']\n",
        "labels1 = league_train1_full['Unnamed: 2']\n",
        "messages1 = list(messages1)\n",
        "labels1 = np.asarray(labels1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svC_pFI23DBZ"
      },
      "source": [
        "league_test1_full = pd.read_csv(\"league_test1.csv\", skiprows=[1], usecols=['Unnamed: 0', 'Unnamed: 1'])\n",
        "test_messages1 = league_test1_full['Unnamed: 0']\n",
        "test_labels1 = league_test1_full['Unnamed: 1']\n",
        "test_labels1 = np.asarray(test_labels1)\n",
        "test_messages1 = list(test_messages1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KaYamepBICT"
      },
      "source": [
        "# Processing Frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2INCa8stBTHp"
      },
      "source": [
        "\n",
        "\n",
        "*   Create dictionary of frequently used phrases in messages classified with class 1 or 2\n",
        "*   Transform messages into frequency matrices\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DocvwkTWdTra"
      },
      "source": [
        "**Dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUKTn7V5da95"
      },
      "source": [
        "toxic_phrases = {'unskilled': 0, 'useless': 1, 'fucking': 2, 'motherfucking': 3, 'noob': 4, 'slut': 5, 'fuck': 6, 'fked': 7, 'fucked': 8, 'miserable': 9, 'sobad': 10, 'youaresobad': 11, \n",
        "                 'doesnothing': 12, 'somad': 13, 'scumbag': 14, 'fu': 15, 'youlost': 16, 'youdidthis': 17, 'goplay': 18, 'barbie': 19, 'screw': 20, 'usuck': 21, 'yousuck': 22,\n",
        "                 'shit': 23, 'wakeup': 24, 'uninstall': 25, 'justuninstall': 26, 'retard': 27, 'tard': 28, 'yourmom': 29, 'urmom': 30, 'drugs': 31, 'justleave': 32, 'muted': 33, \n",
        "                 'shitted': 34, 'gay': 35, 'pussy': 36, 'surrend': 37, 'english': 38, 'stfu': 39, 'playyourself': 40, 'ignore': 41, 'twat': 42, 'wrong': 43, 'bronze': 44, 'nooob': 45, \n",
        "                 'suck': 46, 'fuuck': 47, 'worstteam': 48, 'udone': 49, 'uarebad': 50, 'youarebad': 51, 'ubelongin': 52, 'youbelongin': 53, 'shutup': 54, 'fail': 55, 'fuckyou': 56, \n",
        "                 'fucku': 57, 'crap': 58, 'fucker': 59, 'ihate': 60, 'stupid': 61, 'mother': 62, 'gohome': 63, 'youblind': 64, 'kys': 65, 'lowelo': 66, 'idiot': 67, 'trash': 68, \n",
        "                 'fag': 69, 'fking': 70, 'suckhard': 71, 'uneedhelp': 72, 'youneedhelp': 73, 'didnothing': 74, 'hateu': 75, 'hateyou': 76, 'shame': 77, 'bitch': 78, 'worst': 79, \n",
        "                 'biggestnoob': 80, 'pugface': 81, 'fuking': 82, 'udumb': 83, 'fkin': 84, 'die': 85, 'uninstallpls': 86, 'dick': 87, 'suck': 88, 'brainless': 89, 'neverplay': 90, \n",
        "                 'fuckoff': 91, 'nigger': 92, 'yourdad': 93, 'cock': 94, 'godie': 95, 'aids': 96, 'african': 97, 'fat': 98, 'dumbass': 99, 'pile': 100, 'poop': 101, 'nigga': 102, \n",
        "                 'poo': 103, 'ass': 104, 'nub': 105, 'whatanoob': 106, 'goingafk': 107, 'ufeeder': 108, 'stoptalkin': 109, 'braindead': 110, 'wrongwithu': 111, 'justbad': 112, \n",
        "                 'youtried': 113, 'pathetic': 114, 'awful': 115, 'fcking': 116, 'fck': 117, 'cantplay': 118, 'moron': 119, 'goafk': 120, 'shutit': 121, 'justmute': 122, 'whore': 123, \n",
        "                 'muteher': 124, 'mutehim': 125, 'feeder': 126, 'yourfault': 127, 'betterthanyou': 128, 'piss': 129, 'fuckingnoob': 130, 'yourmother': 131, 'delete': 132, 'suckhard': 133, \n",
        "                 'noskill': 134, 'girl': 135, 'lowlife': 136, 'dontlikeyou': 137, 'dontlikeu': 138, 'pieceofshit': 139, 'badplayer': 140, 'fucktard': 141, 'getlost': 142, 'getcarried': 143, \n",
        "                 'gtfo': 144, 'mental': 145, 'youdont': 146, 'sucksohard': 147, 'youcanmute': 148, 'shutthefuckup': 149, 'puss': 150, 'liar': 151, 'faggot': 152, 'shesbad': 153, 'sucking': 154, \n",
        "                 'lowlevel': 155, 'bequiet': 156, 'asshole': 157, 'cry': 158, 'prostitute': 159, 'german': 160, 'youfeeder': 161, 'asianbitch': 162, 'juststop': 163, 'fuman': 164, 'tetris': 165}\n",
        "toxic_phrase_list = []\n",
        "for word in toxic_phrases:\n",
        "  toxic_phrase_list.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0RHw3ei4v2s"
      },
      "source": [
        "def process_message(message):\n",
        "  \"\"\"\n",
        "  Normalize a message by removing spaces and converting to lowercase.\n",
        "  \"\"\"\n",
        "  no_spaces = message.replace(\" \", \"\")\n",
        "  return no_spaces.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-IapgYN7iKn"
      },
      "source": [
        "def transform_text(messages, phrase_list):\n",
        "    \"\"\"\n",
        "    This function should create a numpy array that contains the number of times each phrase\n",
        "    of the dictionary appears in each message. \n",
        "    Each row in the resulting array should correspond to each message \n",
        "    and each column should correspond to a word of the vocabulary.\n",
        "\n",
        "    Use the provided word dictionary to map words to column indices. Ignore words that\n",
        "    are not present in the dictionary. Use process_message on each message.\n",
        "\n",
        "    Args:\n",
        "        messages: A list of strings where each string is a chat message.\n",
        "        word_dictionary: A python dict mapping phrases to integers.\n",
        "\n",
        "    Returns:\n",
        "        A numpy array marking the words present in each message.\n",
        "        Where the component (i,j) is the number of occurrences of the\n",
        "        j-th vocabulary phrase in the i-th message.\n",
        "    \"\"\"\n",
        "    arr = np.zeros((len(messages), len(phrase_list)))\n",
        "    for msg_idx in range(len(messages)):\n",
        "        processed = process_message(messages[msg_idx])\n",
        "        for phrase in phrase_list:\n",
        "          if phrase in processed:\n",
        "            count_occurr = processed.count(phrase)\n",
        "            arr[msg_idx, phrase_list.index(phrase)] += (count_occurr)\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh6T2J-jeB4y"
      },
      "source": [
        "**Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivB1qzNPeKBD"
      },
      "source": [
        "train_matrix = transform_text(messages1, toxic_phrase_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il2doXIGiWyY"
      },
      "source": [
        "# Multinomial Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGoSD9nHlHiZ"
      },
      "source": [
        "\n",
        "\n",
        "*   Calculate model parameters\n",
        "*   Add Laplace smoothing\n",
        "*   Compute accuracies on test sets\n",
        "*   Try on different training sets; compare parameters\n",
        "*   Try different thresholds for predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4kS9NH3iaDH"
      },
      "source": [
        "def fit_naive_bayes_model(matrix, labels):\n",
        "    \"\"\"\n",
        "    Return the model parameters (likelihood of each word given class label,\n",
        "    prior probabilities)\n",
        "\n",
        "    Args:\n",
        "        matrix: A numpy array containing word counts for the training data\n",
        "        labels: A vector with labels for the training data\n",
        "\n",
        "    Returns: The trained model parameters\n",
        "    \"\"\"\n",
        "    V = matrix.shape[1] # number of words in dictionary\n",
        "    N = matrix.shape[0] # number of total messages\n",
        "    neutral_messages = matrix[labels == 0]\n",
        "    toxic_messages = matrix[labels == 1]\n",
        "    derog_messages = matrix[labels == 2]\n",
        "\n",
        "    # find likelihood parameters: probabilities of each word given the class label\n",
        "    likelihoods_class0 = np.zeros((V,))\n",
        "    likelihoods_class1 = np.zeros((V,))\n",
        "    likelihoods_class2 = np.zeros((V,))\n",
        "    priors = []\n",
        "\n",
        "    n_neutral_words = np.sum(neutral_messages)\n",
        "    n_toxic_words = np.sum(toxic_messages)\n",
        "    n_derog_words = np.sum(derog_messages)\n",
        "    for w in range(V):\n",
        "        likelihoods_class0[w] = (sum(neutral_messages[:,w]) + 1)/ (n_neutral_words + V)\n",
        "        likelihoods_class1[w] = (sum(toxic_messages[:,w]) + 1)/ (n_toxic_words + V)\n",
        "        likelihoods_class2[w] = (sum(derog_messages[:,w]) + 1)/ (n_derog_words + V)\n",
        "\n",
        "    priors.append(neutral_messages.shape[0] / N)\n",
        "    priors.append(toxic_messages.shape[0] / N)\n",
        "    priors.append(derog_messages.shape[0] / N)\n",
        "\n",
        "    return [likelihoods_class0, likelihoods_class1, likelihoods_class2, priors]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k23lSpg2FGP"
      },
      "source": [
        "def predict_from_naive_bayes_model(messages, model, matrix):\n",
        "    \"\"\"Use a Naive Bayes model to compute predictions for a target matrix.\n",
        "\n",
        "    This function should be able to predict on the models that fit_naive_bayes_model\n",
        "    outputs.\n",
        "\n",
        "    Args:\n",
        "        model: A trained model from fit_naive_bayes_model\n",
        "        matrix: A numpy array containing word counts\n",
        "\n",
        "    Returns: A numpy array containing the predictions from the model (int array of 0 or 1 values)\n",
        "    \"\"\"\n",
        "    likelihoods_class0 = model[0]\n",
        "    likelihoods_class1 = model[1]\n",
        "    likelihoods_class2 = model[2]\n",
        "    priors = model[3]\n",
        "    V = matrix.shape[1]\n",
        "    N = matrix.shape[0]\n",
        "    log_prior0 = math.log(priors[0])\n",
        "    log_prior1 = math.log(priors[1])\n",
        "    log_prior2 = math.log(priors[2])\n",
        "    preds = np.zeros((N,))\n",
        "    for i in range(N):\n",
        "        sum_class0 = 0\n",
        "        sum_class1 = 0\n",
        "        sum_class2 = 0\n",
        "        for t in range(V):\n",
        "            sum_class0 += matrix[i, t] * math.log(likelihoods_class0[t])\n",
        "            sum_class1 += matrix[i, t] * math.log(likelihoods_class1[t])\n",
        "            sum_class2 += matrix[i, t] * math.log(likelihoods_class2[t])\n",
        "        prob_class0 = log_prior0 + sum_class0\n",
        "        prob_class1 = log_prior1 + sum_class1\n",
        "        prob_class2 = log_prior2 + sum_class2\n",
        "        probs = [prob_class0, prob_class1, prob_class2]\n",
        "        class_pred = probs.index(max(probs))\n",
        "        preds[i] = class_pred\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4R4BqU5kdK6"
      },
      "source": [
        "# run on processed matrix\n",
        "params = fit_naive_bayes_model(train_matrix, labels1)\n",
        "# save predicted outputs to a file\n",
        "test_matrix = transform_text(test_messages1, toxic_phrase_list)\n",
        "preds = predict_from_naive_bayes_model(test_messages1, params, test_matrix)\n",
        "np.savetxt('predictions.txt', preds)\n",
        "naive_bayes_accuracy = np.mean(preds == test_labels1)\n",
        "print(naive_bayes_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6baLXl-8Ssf"
      },
      "source": [
        "confusion_matrix(test_labels1, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzooozbfC6Zb"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(f1_score(test_labels1, preds, average=None))\n",
        "print(confusion_matrix(test_labels1, preds, normalize='true'))\n",
        "print(\"Accuracy:\",metrics.accuracy_score(test_labels1, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR1XV5oiD0wt"
      },
      "source": [
        "def get_top_five_naive_bayes_words(model, dictionary):\n",
        "    \"\"\"Compute the top five words that are most indicative of the spam (i.e positive) class.\n",
        "\n",
        "    Ues the metric given in part-c as a measure of how indicative a word is.\n",
        "    Return the words in sorted form, with the most indicative word first.\n",
        "\n",
        "    Args:\n",
        "        model: The Naive Bayes model returned from fit_naive_bayes_model\n",
        "        dictionary: A mapping of word to integer ids\n",
        "\n",
        "    Returns: A list of the top five most indicative words in sorted order with the most indicative first\n",
        "    \"\"\"\n",
        "    top_five = []\n",
        "    V = len(dictionary)\n",
        "    likelihoods_class0 = model[0]\n",
        "    likelihoods_class1 = model[1]\n",
        "    log_ratios = np.zeros((V,))\n",
        "    key_list = list(dictionary.keys())\n",
        "\n",
        "    for i in range(V):\n",
        "        ratio = likelihoods_class0[i] / likelihoods_class1[i]\n",
        "        log_ratios[i] = math.log(ratio)\n",
        "    sorted_idxs = np.argsort(log_ratios)\n",
        "    top_five_idxs = sorted_idxs[-5:]\n",
        "    top_five_idxs_sorted = np.flip(top_five_idxs)\n",
        "\n",
        "    for idx in top_five_idxs_sorted:\n",
        "        top_five.append(key_list[idx])\n",
        "    return top_five"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGlObF_SnbIS"
      },
      "source": [
        "get_top_five_naive_bayes_words(params, toxic_phrases)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}