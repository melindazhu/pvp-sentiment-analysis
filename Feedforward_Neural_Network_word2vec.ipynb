{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feedforward Neural Network word2vec.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vf-lqCfM3V1"
      },
      "source": [
        "# Feedforward NN with Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTnP71OAOxuG"
      },
      "source": [
        "Sources:\n",
        "\n",
        "How to embed with Word2Vec - https://www.kaggle.com/vladislavkisin/word2vec-in-supervised-nlp-tasks-shortcut\n",
        "\n",
        "Code reference: Deep Learning with Python (Chollet) see report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEPuuOA8Mwrz"
      },
      "source": [
        "Imports and loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuNtnjnsYFRQ"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "import re\n",
        "import numpy as np\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2IpeubkbAGC"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOFPwaLsYStj"
      },
      "source": [
        "df = pd.read_csv('train1_full.csv') \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzrcBXLrZL0Z"
      },
      "source": [
        "# Here we get transform the documents into sentences for the word2vecmodel\n",
        "def preprocess(df):\n",
        "    df['comment_text'] = df.comment_text.str.lower()\n",
        "    df['document_sentences'] = df.comment_text.str.split('.') \n",
        "    df['tokenized_sentences'] = list(map(lambda sentences: list(map(nltk.word_tokenize, sentences)), df.document_sentences))  \n",
        "    df['tokenized_sentences'] = list(map(lambda sentences: list(filter(lambda lst: lst, sentences)), df.tokenized_sentences))\n",
        "\n",
        "preprocess(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-RzvtoSaI0c"
      },
      "source": [
        "Split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlG5SC_EZWU6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test, y_train, y_test = train_test_split(df.drop(columns='label'), df['label'], test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9cIgWYjlyvO"
      },
      "source": [
        "def remove_items(test_list, item):\n",
        "    # utility function to remove stop words\n",
        "    for i in test_list:\n",
        "        if(i == item):\n",
        "            test_list.remove(i)\n",
        "  \n",
        "    return test_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_ovXsHol0ez"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    sentence = sen.lower()\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    sentence_list = sentence.split()\n",
        "\n",
        "    # Removing stop words\n",
        "    stop_words = ['u', 'ur', 'im', 'can', 'cant', 'i', 'me', 'my', 'myself', 'we', 'go', 'our', 'ours', 'ourselves', 'you', \"youre\", \"youve\", \"youll\", \"youd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"thatll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"shouldve\", 'now', 'd', 'll', 'm', 'o', 're', 'r', 'ur', 've', 'y', 'ain', 'aren', \"arent\", 'couldn', \"couldnt\", 'didn', \"didnt\", 'doesn', \"doesnt\", 'hadn', \"hadnt\", 'hasn', \"hasnt\", 'haven', \"havent\", 'isn', \"isnt\", 'ma', 'mightn', \"mightnt\", 'mustn', \"mustnt\", 'needn', \"neednt\", 'shan', \"shant\", 'shouldn', \"shouldnt\", 'wasn', \"wasnt\", 'weren', \"werent\", 'won', \"wont\", 'wouldn', \"wouldnt\"]\n",
        "    for stop_word in stop_words:\n",
        "      if stop_word in sentence_list:\n",
        "        sentence_list = remove_items(sentence_list, stop_word)\n",
        "\n",
        "    # Join back to list\n",
        "    sentence = \" \".join(sentence_list)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    return sentence.lstrip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcq-m6qHNpxn"
      },
      "source": [
        "Get a list of all messages and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1NSdlRdl3UK"
      },
      "source": [
        "X_train = []\n",
        "sentences = list(train[\"comment_text\"])\n",
        "for sen in sentences:\n",
        "    X_train.append(preprocess_text(sen))\n",
        "\n",
        "from keras.utils.np_utils import to_categorical   \n",
        "categorical_labels = to_categorical(y_train, num_classes=3) # this changes the labels to one-hot encodings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UecOcQczmm3K"
      },
      "source": [
        "X_test = []\n",
        "sentences1 = list(test[\"comment_text\"])\n",
        "for sen in sentences1:\n",
        "    X_test.append(preprocess_text(sen))\n",
        "  \n",
        "test_categorical_labels = to_categorical(y_test, num_classes=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tn40FYcNxqa"
      },
      "source": [
        "# Tokenizing and Word2Vec Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_VT7rCyN1IZ"
      },
      "source": [
        "Use the Tokenizer class to convert the sentences into matrices (the X_train and X_test matrices)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHcJeR6em2fL"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "maxlen = 10\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test_tokens = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwM0qberaI3n"
      },
      "source": [
        "Collect a vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4QNh9EBevUy"
      },
      "source": [
        "def remove_items(test_list, item):\n",
        "    # remove the item for all its occurrences\n",
        "    for i in test_list:\n",
        "        if(i == item):\n",
        "            test_list.remove(i)\n",
        "  \n",
        "    return test_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "berG1Ir-aEEq"
      },
      "source": [
        "#Collecting a vocabulary\n",
        "voc = []\n",
        "# top = stopwords.words('english')\n",
        "for sentence in train.tokenized_sentences:\n",
        "    # if len(sentence) == 0:\n",
        "    #   continue\n",
        "    # else:\n",
        "    #   for word in stop:\n",
        "    #       sentence[0] = remove_items(sentence[0], word)\n",
        "    voc.extend(sentence)\n",
        "\n",
        "print(\"Number of sentences: {}.\".format(len(voc)))\n",
        "print(\"Number of rows: {}.\".format(len(train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8bUXhDjOEiH"
      },
      "source": [
        "Use the Word2Vec embeddings, which will be used in the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWs9HFgceIDm"
      },
      "source": [
        "from gensim.models import word2vec, Word2Vec\n",
        "\n",
        "num_features = 50\n",
        "min_word_count = 1   \n",
        "num_workers = 4       \n",
        "context = 1        \n",
        "downsampling = 1e-3   \n",
        "\n",
        "# Initialize and train the model\n",
        "W2Vmodel = Word2Vec(sentences=voc, sg=1, hs=0, workers=num_workers, size=num_features, min_count=min_word_count, window=context,\n",
        "                    sample=downsampling, negative=5, iter=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8vQL2DqfgLu"
      },
      "source": [
        "def sentence_vectors(model, sentence):\n",
        "    #Collecting all words in the text\n",
        "    words = []\n",
        "    if len(sentence) != 0:\n",
        "      words=np.concatenate(sentence)\n",
        "    #Collecting words that are known to the model\n",
        "    model_voc = set(model.wv.vocab.keys()) \n",
        "    \n",
        "    sent_vector = np.zeros(model.vector_size, dtype=\"float32\")\n",
        "    \n",
        "    # Use a counter variable for number of words in a text\n",
        "    nwords = 0\n",
        "    # Sum up all words vectors that are know to the model\n",
        "    for word in words:\n",
        "        if word in model_voc: \n",
        "            sent_vector += model[word]\n",
        "            nwords += 1.\n",
        "\n",
        "    # Now get the average\n",
        "    if nwords > 0:\n",
        "        sent_vector /= nwords\n",
        "    return sent_vector\n",
        "\n",
        "train['sentence_vectors'] = list(map(lambda sen_group:\n",
        "                                      sentence_vectors(W2Vmodel, sen_group),\n",
        "                                      train.tokenized_sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8fbxCuROLgu"
      },
      "source": [
        "Create the embeddings matrix that will be entered into the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX7Q4-vbfiqv"
      },
      "source": [
        "def vectors_to_feats(df, ndim):\n",
        "    index=[]\n",
        "    for i in range(ndim):\n",
        "        df[f'w2v_{i}'] = df['sentence_vectors'].apply(lambda x: x[i])\n",
        "        index.append(f'w2v_{i}')\n",
        "    return df[index]\n",
        "embeddings = vectors_to_feats(train, 50)\n",
        "embeddings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfOjrOcMhSYN"
      },
      "source": [
        "test['sentence_vectors'] = list(map(lambda sen_group:sentence_vectors(W2Vmodel, sen_group), test.tokenized_sentences))\n",
        "X_test=vectors_to_feats(test, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SKKy9GLOU2Q"
      },
      "source": [
        "# Creating the Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNBEj_pNOYEk"
      },
      "source": [
        "Specify the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDcHh7kBn7Dc"
      },
      "source": [
        "maxlen = 10\n",
        "from keras import layers\n",
        "from keras import models\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(100, activation='relu', input_shape=(50,)))\n",
        "model.add(layers.Dense(50, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1Ehcd1VeAY0"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbrO0zlHeb1P"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2HPAsOYOhrU"
      },
      "source": [
        "# Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubwfCBA8OcY0"
      },
      "source": [
        "Train the model for 300 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fuNDV0QgzYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47147c04-d303-4608-bdd5-e8c008951575"
      },
      "source": [
        "history = model.fit(embeddings, categorical_labels, batch_size=30, epochs=550, verbose=1, validation_split=0.20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/550\n",
            "688/688 [==============================] - 2s 2ms/step - loss: 0.7233 - acc: 0.6923 - val_loss: 0.5848 - val_acc: 0.7724\n",
            "Epoch 2/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.5921 - acc: 0.7610 - val_loss: 0.5852 - val_acc: 0.7645\n",
            "Epoch 3/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.5769 - acc: 0.7654 - val_loss: 0.5518 - val_acc: 0.7788\n",
            "Epoch 4/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.5618 - acc: 0.7690 - val_loss: 0.5380 - val_acc: 0.7916\n",
            "Epoch 5/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.5519 - acc: 0.7749 - val_loss: 0.5338 - val_acc: 0.7941\n",
            "Epoch 6/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.5467 - acc: 0.7752 - val_loss: 0.5226 - val_acc: 0.7905\n",
            "Epoch 7/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.5327 - acc: 0.7816 - val_loss: 0.5068 - val_acc: 0.8007\n",
            "Epoch 8/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.5325 - acc: 0.7812 - val_loss: 0.5113 - val_acc: 0.7953\n",
            "Epoch 9/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.5215 - acc: 0.7888 - val_loss: 0.5086 - val_acc: 0.7873\n",
            "Epoch 10/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.5094 - acc: 0.7928 - val_loss: 0.4918 - val_acc: 0.8050\n",
            "Epoch 11/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4946 - acc: 0.8018 - val_loss: 0.4907 - val_acc: 0.8023\n",
            "Epoch 12/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.5031 - acc: 0.7943 - val_loss: 0.4924 - val_acc: 0.7957\n",
            "Epoch 13/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4940 - acc: 0.7971 - val_loss: 0.5011 - val_acc: 0.7852\n",
            "Epoch 14/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4841 - acc: 0.8034 - val_loss: 0.4687 - val_acc: 0.8128\n",
            "Epoch 15/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4917 - acc: 0.8002 - val_loss: 0.4691 - val_acc: 0.8128\n",
            "Epoch 16/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4693 - acc: 0.8050 - val_loss: 0.4694 - val_acc: 0.8079\n",
            "Epoch 17/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4842 - acc: 0.8003 - val_loss: 0.4690 - val_acc: 0.8058\n",
            "Epoch 18/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4613 - acc: 0.8136 - val_loss: 0.4772 - val_acc: 0.7986\n",
            "Epoch 19/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4613 - acc: 0.8127 - val_loss: 0.4567 - val_acc: 0.8114\n",
            "Epoch 20/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4592 - acc: 0.8129 - val_loss: 0.4661 - val_acc: 0.8066\n",
            "Epoch 21/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4508 - acc: 0.8156 - val_loss: 0.4698 - val_acc: 0.8079\n",
            "Epoch 22/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4563 - acc: 0.8148 - val_loss: 0.4473 - val_acc: 0.8186\n",
            "Epoch 23/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4455 - acc: 0.8221 - val_loss: 0.4465 - val_acc: 0.8186\n",
            "Epoch 24/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4400 - acc: 0.8230 - val_loss: 0.4452 - val_acc: 0.8192\n",
            "Epoch 25/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4447 - acc: 0.8204 - val_loss: 0.4387 - val_acc: 0.8246\n",
            "Epoch 26/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4365 - acc: 0.8198 - val_loss: 0.4394 - val_acc: 0.8188\n",
            "Epoch 27/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4332 - acc: 0.8252 - val_loss: 0.4389 - val_acc: 0.8263\n",
            "Epoch 28/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4307 - acc: 0.8252 - val_loss: 0.4374 - val_acc: 0.8215\n",
            "Epoch 29/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4422 - acc: 0.8184 - val_loss: 0.4290 - val_acc: 0.8242\n",
            "Epoch 30/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4319 - acc: 0.8249 - val_loss: 0.4293 - val_acc: 0.8271\n",
            "Epoch 31/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4299 - acc: 0.8252 - val_loss: 0.4271 - val_acc: 0.8267\n",
            "Epoch 32/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4190 - acc: 0.8325 - val_loss: 0.4417 - val_acc: 0.8176\n",
            "Epoch 33/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.8287 - val_loss: 0.4690 - val_acc: 0.7982\n",
            "Epoch 34/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4202 - acc: 0.8287 - val_loss: 0.4568 - val_acc: 0.8114\n",
            "Epoch 35/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4146 - acc: 0.8280 - val_loss: 0.4370 - val_acc: 0.8238\n",
            "Epoch 36/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4141 - acc: 0.8311 - val_loss: 0.4440 - val_acc: 0.8145\n",
            "Epoch 37/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4022 - acc: 0.8368 - val_loss: 0.4164 - val_acc: 0.8293\n",
            "Epoch 38/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4000 - acc: 0.8385 - val_loss: 0.4511 - val_acc: 0.8135\n",
            "Epoch 39/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4082 - acc: 0.8329 - val_loss: 0.4481 - val_acc: 0.8170\n",
            "Epoch 40/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4050 - acc: 0.8329 - val_loss: 0.4081 - val_acc: 0.8316\n",
            "Epoch 41/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3984 - acc: 0.8366 - val_loss: 0.4283 - val_acc: 0.8231\n",
            "Epoch 42/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4117 - acc: 0.8303 - val_loss: 0.4128 - val_acc: 0.8304\n",
            "Epoch 43/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3916 - acc: 0.8396 - val_loss: 0.4202 - val_acc: 0.8329\n",
            "Epoch 44/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3953 - acc: 0.8400 - val_loss: 0.4177 - val_acc: 0.8238\n",
            "Epoch 45/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.4056 - acc: 0.8372 - val_loss: 0.4115 - val_acc: 0.8296\n",
            "Epoch 46/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3799 - acc: 0.8459 - val_loss: 0.4294 - val_acc: 0.8176\n",
            "Epoch 47/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3898 - acc: 0.8412 - val_loss: 0.4057 - val_acc: 0.8337\n",
            "Epoch 48/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3851 - acc: 0.8430 - val_loss: 0.4152 - val_acc: 0.8312\n",
            "Epoch 49/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3804 - acc: 0.8430 - val_loss: 0.4026 - val_acc: 0.8331\n",
            "Epoch 50/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3843 - acc: 0.8432 - val_loss: 0.4159 - val_acc: 0.8333\n",
            "Epoch 51/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3694 - acc: 0.8491 - val_loss: 0.4146 - val_acc: 0.8341\n",
            "Epoch 52/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3832 - acc: 0.8459 - val_loss: 0.4314 - val_acc: 0.8198\n",
            "Epoch 53/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3982 - acc: 0.8375 - val_loss: 0.4246 - val_acc: 0.8260\n",
            "Epoch 54/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3832 - acc: 0.8463 - val_loss: 0.4232 - val_acc: 0.8250\n",
            "Epoch 55/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3677 - acc: 0.8504 - val_loss: 0.3915 - val_acc: 0.8388\n",
            "Epoch 56/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3781 - acc: 0.8451 - val_loss: 0.4184 - val_acc: 0.8248\n",
            "Epoch 57/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3667 - acc: 0.8502 - val_loss: 0.4221 - val_acc: 0.8269\n",
            "Epoch 58/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3629 - acc: 0.8518 - val_loss: 0.4053 - val_acc: 0.8314\n",
            "Epoch 59/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3766 - acc: 0.8465 - val_loss: 0.3922 - val_acc: 0.8360\n",
            "Epoch 60/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3658 - acc: 0.8470 - val_loss: 0.4001 - val_acc: 0.8347\n",
            "Epoch 61/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3658 - acc: 0.8541 - val_loss: 0.3939 - val_acc: 0.8376\n",
            "Epoch 62/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3591 - acc: 0.8520 - val_loss: 0.3931 - val_acc: 0.8415\n",
            "Epoch 63/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3591 - acc: 0.8521 - val_loss: 0.3820 - val_acc: 0.8411\n",
            "Epoch 64/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3589 - acc: 0.8523 - val_loss: 0.3846 - val_acc: 0.8436\n",
            "Epoch 65/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3540 - acc: 0.8549 - val_loss: 0.3894 - val_acc: 0.8446\n",
            "Epoch 66/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3557 - acc: 0.8549 - val_loss: 0.4575 - val_acc: 0.8097\n",
            "Epoch 67/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3585 - acc: 0.8508 - val_loss: 0.4083 - val_acc: 0.8359\n",
            "Epoch 68/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3594 - acc: 0.8545 - val_loss: 0.3979 - val_acc: 0.8335\n",
            "Epoch 69/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3444 - acc: 0.8617 - val_loss: 0.3858 - val_acc: 0.8469\n",
            "Epoch 70/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3639 - acc: 0.8517 - val_loss: 0.3785 - val_acc: 0.8432\n",
            "Epoch 71/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3446 - acc: 0.8605 - val_loss: 0.4630 - val_acc: 0.8161\n",
            "Epoch 72/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3491 - acc: 0.8552 - val_loss: 0.3981 - val_acc: 0.8372\n",
            "Epoch 73/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3559 - acc: 0.8595 - val_loss: 0.3732 - val_acc: 0.8522\n",
            "Epoch 74/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3565 - acc: 0.8546 - val_loss: 0.3837 - val_acc: 0.8448\n",
            "Epoch 75/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3458 - acc: 0.8596 - val_loss: 0.3712 - val_acc: 0.8539\n",
            "Epoch 76/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3373 - acc: 0.8628 - val_loss: 0.3751 - val_acc: 0.8496\n",
            "Epoch 77/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3345 - acc: 0.8672 - val_loss: 0.3753 - val_acc: 0.8487\n",
            "Epoch 78/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3473 - acc: 0.8564 - val_loss: 0.3748 - val_acc: 0.8477\n",
            "Epoch 79/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3330 - acc: 0.8658 - val_loss: 0.3702 - val_acc: 0.8522\n",
            "Epoch 80/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3261 - acc: 0.8658 - val_loss: 0.3780 - val_acc: 0.8465\n",
            "Epoch 81/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3387 - acc: 0.8634 - val_loss: 0.3705 - val_acc: 0.8483\n",
            "Epoch 82/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3306 - acc: 0.8634 - val_loss: 0.3839 - val_acc: 0.8477\n",
            "Epoch 83/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3263 - acc: 0.8682 - val_loss: 0.3843 - val_acc: 0.8456\n",
            "Epoch 84/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3277 - acc: 0.8668 - val_loss: 0.3648 - val_acc: 0.8547\n",
            "Epoch 85/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3349 - acc: 0.8632 - val_loss: 0.3776 - val_acc: 0.8529\n",
            "Epoch 86/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3199 - acc: 0.8708 - val_loss: 0.4236 - val_acc: 0.8256\n",
            "Epoch 87/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3246 - acc: 0.8681 - val_loss: 0.3644 - val_acc: 0.8520\n",
            "Epoch 88/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3328 - acc: 0.8668 - val_loss: 0.3722 - val_acc: 0.8483\n",
            "Epoch 89/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3166 - acc: 0.8742 - val_loss: 0.3805 - val_acc: 0.8434\n",
            "Epoch 90/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3188 - acc: 0.8709 - val_loss: 0.3554 - val_acc: 0.8510\n",
            "Epoch 91/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3120 - acc: 0.8733 - val_loss: 0.3678 - val_acc: 0.8473\n",
            "Epoch 92/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3128 - acc: 0.8715 - val_loss: 0.3955 - val_acc: 0.8428\n",
            "Epoch 93/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3228 - acc: 0.8666 - val_loss: 0.3829 - val_acc: 0.8397\n",
            "Epoch 94/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3064 - acc: 0.8763 - val_loss: 0.3753 - val_acc: 0.8401\n",
            "Epoch 95/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3201 - acc: 0.8700 - val_loss: 0.3840 - val_acc: 0.8403\n",
            "Epoch 96/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3210 - acc: 0.8699 - val_loss: 0.4058 - val_acc: 0.8318\n",
            "Epoch 97/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3212 - acc: 0.8694 - val_loss: 0.3579 - val_acc: 0.8560\n",
            "Epoch 98/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3120 - acc: 0.8684 - val_loss: 0.3489 - val_acc: 0.8549\n",
            "Epoch 99/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3263 - acc: 0.8624 - val_loss: 0.3517 - val_acc: 0.8578\n",
            "Epoch 100/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2941 - acc: 0.8848 - val_loss: 0.3677 - val_acc: 0.8487\n",
            "Epoch 101/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2986 - acc: 0.8792 - val_loss: 0.3721 - val_acc: 0.8465\n",
            "Epoch 102/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3234 - acc: 0.8718 - val_loss: 0.3575 - val_acc: 0.8560\n",
            "Epoch 103/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3050 - acc: 0.8729 - val_loss: 0.3439 - val_acc: 0.8669\n",
            "Epoch 104/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2964 - acc: 0.8808 - val_loss: 0.3553 - val_acc: 0.8555\n",
            "Epoch 105/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2923 - acc: 0.8819 - val_loss: 0.3659 - val_acc: 0.8465\n",
            "Epoch 106/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.3037 - acc: 0.8778 - val_loss: 0.3560 - val_acc: 0.8572\n",
            "Epoch 107/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2939 - acc: 0.8793 - val_loss: 0.3412 - val_acc: 0.8636\n",
            "Epoch 108/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2934 - acc: 0.8853 - val_loss: 0.3535 - val_acc: 0.8574\n",
            "Epoch 109/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2946 - acc: 0.8811 - val_loss: 0.3509 - val_acc: 0.8615\n",
            "Epoch 110/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2951 - acc: 0.8814 - val_loss: 0.3454 - val_acc: 0.8551\n",
            "Epoch 111/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2843 - acc: 0.8867 - val_loss: 0.3381 - val_acc: 0.8622\n",
            "Epoch 112/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2938 - acc: 0.8823 - val_loss: 0.3490 - val_acc: 0.8560\n",
            "Epoch 113/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2931 - acc: 0.8834 - val_loss: 0.3358 - val_acc: 0.8679\n",
            "Epoch 114/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2825 - acc: 0.8846 - val_loss: 0.3315 - val_acc: 0.8690\n",
            "Epoch 115/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2978 - acc: 0.8776 - val_loss: 0.3507 - val_acc: 0.8607\n",
            "Epoch 116/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2844 - acc: 0.8868 - val_loss: 0.3249 - val_acc: 0.8727\n",
            "Epoch 117/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2867 - acc: 0.8829 - val_loss: 0.3577 - val_acc: 0.8574\n",
            "Epoch 118/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2758 - acc: 0.8864 - val_loss: 0.3444 - val_acc: 0.8690\n",
            "Epoch 119/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2832 - acc: 0.8874 - val_loss: 0.3577 - val_acc: 0.8605\n",
            "Epoch 120/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2761 - acc: 0.8897 - val_loss: 0.3791 - val_acc: 0.8518\n",
            "Epoch 121/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2758 - acc: 0.8911 - val_loss: 0.3187 - val_acc: 0.8681\n",
            "Epoch 122/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2717 - acc: 0.8898 - val_loss: 0.3710 - val_acc: 0.8459\n",
            "Epoch 123/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2772 - acc: 0.8881 - val_loss: 0.3412 - val_acc: 0.8646\n",
            "Epoch 124/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2785 - acc: 0.8875 - val_loss: 0.3381 - val_acc: 0.8653\n",
            "Epoch 125/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2672 - acc: 0.8932 - val_loss: 0.3334 - val_acc: 0.8655\n",
            "Epoch 126/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2769 - acc: 0.8841 - val_loss: 0.3147 - val_acc: 0.8774\n",
            "Epoch 127/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2730 - acc: 0.8886 - val_loss: 0.3124 - val_acc: 0.8789\n",
            "Epoch 128/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2662 - acc: 0.8946 - val_loss: 0.3347 - val_acc: 0.8607\n",
            "Epoch 129/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2758 - acc: 0.8872 - val_loss: 0.3312 - val_acc: 0.8665\n",
            "Epoch 130/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2648 - acc: 0.8932 - val_loss: 0.3770 - val_acc: 0.8539\n",
            "Epoch 131/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2665 - acc: 0.8940 - val_loss: 0.3327 - val_acc: 0.8698\n",
            "Epoch 132/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2639 - acc: 0.8943 - val_loss: 0.3302 - val_acc: 0.8663\n",
            "Epoch 133/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2597 - acc: 0.8937 - val_loss: 0.3250 - val_acc: 0.8729\n",
            "Epoch 134/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2608 - acc: 0.8962 - val_loss: 0.3568 - val_acc: 0.8646\n",
            "Epoch 135/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2592 - acc: 0.8974 - val_loss: 0.3547 - val_acc: 0.8595\n",
            "Epoch 136/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2711 - acc: 0.8921 - val_loss: 0.3205 - val_acc: 0.8698\n",
            "Epoch 137/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2685 - acc: 0.8929 - val_loss: 0.3297 - val_acc: 0.8708\n",
            "Epoch 138/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2700 - acc: 0.8920 - val_loss: 0.3048 - val_acc: 0.8836\n",
            "Epoch 139/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2573 - acc: 0.8992 - val_loss: 0.3015 - val_acc: 0.8774\n",
            "Epoch 140/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2529 - acc: 0.9004 - val_loss: 0.3181 - val_acc: 0.8752\n",
            "Epoch 141/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2611 - acc: 0.8958 - val_loss: 0.3271 - val_acc: 0.8745\n",
            "Epoch 142/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2594 - acc: 0.8935 - val_loss: 0.3175 - val_acc: 0.8795\n",
            "Epoch 143/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2528 - acc: 0.8995 - val_loss: 0.3091 - val_acc: 0.8789\n",
            "Epoch 144/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2682 - acc: 0.8944 - val_loss: 0.3104 - val_acc: 0.8782\n",
            "Epoch 145/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2472 - acc: 0.9036 - val_loss: 0.3171 - val_acc: 0.8793\n",
            "Epoch 146/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2451 - acc: 0.9019 - val_loss: 0.3352 - val_acc: 0.8714\n",
            "Epoch 147/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2552 - acc: 0.8985 - val_loss: 0.3372 - val_acc: 0.8671\n",
            "Epoch 148/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2567 - acc: 0.8991 - val_loss: 0.3142 - val_acc: 0.8782\n",
            "Epoch 149/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2413 - acc: 0.9022 - val_loss: 0.3012 - val_acc: 0.8832\n",
            "Epoch 150/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2469 - acc: 0.9008 - val_loss: 0.3265 - val_acc: 0.8770\n",
            "Epoch 151/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2393 - acc: 0.9049 - val_loss: 0.3387 - val_acc: 0.8613\n",
            "Epoch 152/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2531 - acc: 0.9013 - val_loss: 0.3081 - val_acc: 0.8828\n",
            "Epoch 153/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2442 - acc: 0.9025 - val_loss: 0.3088 - val_acc: 0.8828\n",
            "Epoch 154/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2366 - acc: 0.9046 - val_loss: 0.3077 - val_acc: 0.8799\n",
            "Epoch 155/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2347 - acc: 0.9072 - val_loss: 0.2925 - val_acc: 0.8857\n",
            "Epoch 156/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2393 - acc: 0.9055 - val_loss: 0.3093 - val_acc: 0.8805\n",
            "Epoch 157/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2386 - acc: 0.9063 - val_loss: 0.3569 - val_acc: 0.8607\n",
            "Epoch 158/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2446 - acc: 0.9023 - val_loss: 0.3034 - val_acc: 0.8811\n",
            "Epoch 159/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2366 - acc: 0.9111 - val_loss: 0.2892 - val_acc: 0.8869\n",
            "Epoch 160/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2352 - acc: 0.9076 - val_loss: 0.2905 - val_acc: 0.8846\n",
            "Epoch 161/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2426 - acc: 0.9048 - val_loss: 0.3026 - val_acc: 0.8851\n",
            "Epoch 162/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2228 - acc: 0.9139 - val_loss: 0.3260 - val_acc: 0.8716\n",
            "Epoch 163/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2284 - acc: 0.9080 - val_loss: 0.3143 - val_acc: 0.8834\n",
            "Epoch 164/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2381 - acc: 0.9103 - val_loss: 0.3072 - val_acc: 0.8849\n",
            "Epoch 165/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2427 - acc: 0.9066 - val_loss: 0.3014 - val_acc: 0.8851\n",
            "Epoch 166/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2278 - acc: 0.9129 - val_loss: 0.2854 - val_acc: 0.8908\n",
            "Epoch 167/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2630 - acc: 0.8963 - val_loss: 0.2808 - val_acc: 0.8960\n",
            "Epoch 168/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2184 - acc: 0.9183 - val_loss: 0.3090 - val_acc: 0.8789\n",
            "Epoch 169/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2201 - acc: 0.9138 - val_loss: 0.3374 - val_acc: 0.8725\n",
            "Epoch 170/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2273 - acc: 0.9087 - val_loss: 0.3181 - val_acc: 0.8813\n",
            "Epoch 171/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2165 - acc: 0.9142 - val_loss: 0.3087 - val_acc: 0.8822\n",
            "Epoch 172/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2546 - acc: 0.9002 - val_loss: 0.2829 - val_acc: 0.8902\n",
            "Epoch 173/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2183 - acc: 0.9183 - val_loss: 0.3012 - val_acc: 0.8900\n",
            "Epoch 174/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2226 - acc: 0.9119 - val_loss: 0.3142 - val_acc: 0.8844\n",
            "Epoch 175/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2251 - acc: 0.9125 - val_loss: 0.3493 - val_acc: 0.8642\n",
            "Epoch 176/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2221 - acc: 0.9133 - val_loss: 0.3126 - val_acc: 0.8865\n",
            "Epoch 177/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2185 - acc: 0.9135 - val_loss: 0.3260 - val_acc: 0.8737\n",
            "Epoch 178/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2732 - acc: 0.8983 - val_loss: 0.2884 - val_acc: 0.8962\n",
            "Epoch 179/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2171 - acc: 0.9171 - val_loss: 0.3045 - val_acc: 0.8818\n",
            "Epoch 180/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2158 - acc: 0.9141 - val_loss: 0.2938 - val_acc: 0.8855\n",
            "Epoch 181/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2181 - acc: 0.9164 - val_loss: 0.3015 - val_acc: 0.8838\n",
            "Epoch 182/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2323 - acc: 0.9117 - val_loss: 0.3004 - val_acc: 0.8847\n",
            "Epoch 183/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2029 - acc: 0.9209 - val_loss: 0.2990 - val_acc: 0.8880\n",
            "Epoch 184/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2237 - acc: 0.9131 - val_loss: 0.2954 - val_acc: 0.8950\n",
            "Epoch 185/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2114 - acc: 0.9153 - val_loss: 0.2925 - val_acc: 0.8900\n",
            "Epoch 186/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2063 - acc: 0.9211 - val_loss: 0.2686 - val_acc: 0.9005\n",
            "Epoch 187/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2096 - acc: 0.9185 - val_loss: 0.2842 - val_acc: 0.8923\n",
            "Epoch 188/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2055 - acc: 0.9209 - val_loss: 0.2761 - val_acc: 0.8985\n",
            "Epoch 189/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2035 - acc: 0.9219 - val_loss: 0.3245 - val_acc: 0.8834\n",
            "Epoch 190/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2148 - acc: 0.9186 - val_loss: 0.2795 - val_acc: 0.8910\n",
            "Epoch 191/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1998 - acc: 0.9242 - val_loss: 0.3149 - val_acc: 0.8830\n",
            "Epoch 192/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2147 - acc: 0.9180 - val_loss: 0.3229 - val_acc: 0.8762\n",
            "Epoch 193/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2112 - acc: 0.9171 - val_loss: 0.2731 - val_acc: 0.8974\n",
            "Epoch 194/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1981 - acc: 0.9257 - val_loss: 0.2936 - val_acc: 0.8884\n",
            "Epoch 195/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2052 - acc: 0.9200 - val_loss: 0.2906 - val_acc: 0.8880\n",
            "Epoch 196/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1931 - acc: 0.9264 - val_loss: 0.2726 - val_acc: 0.8958\n",
            "Epoch 197/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1975 - acc: 0.9248 - val_loss: 0.3001 - val_acc: 0.8853\n",
            "Epoch 198/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2067 - acc: 0.9240 - val_loss: 0.3030 - val_acc: 0.8884\n",
            "Epoch 199/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1977 - acc: 0.9251 - val_loss: 0.2894 - val_acc: 0.9003\n",
            "Epoch 200/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1962 - acc: 0.9250 - val_loss: 0.2824 - val_acc: 0.8950\n",
            "Epoch 201/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1924 - acc: 0.9282 - val_loss: 0.3116 - val_acc: 0.8865\n",
            "Epoch 202/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1959 - acc: 0.9239 - val_loss: 0.2796 - val_acc: 0.8871\n",
            "Epoch 203/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2042 - acc: 0.9188 - val_loss: 0.3027 - val_acc: 0.8877\n",
            "Epoch 204/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1990 - acc: 0.9220 - val_loss: 0.2899 - val_acc: 0.8818\n",
            "Epoch 205/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1975 - acc: 0.9240 - val_loss: 0.2910 - val_acc: 0.8912\n",
            "Epoch 206/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1914 - acc: 0.9276 - val_loss: 0.2650 - val_acc: 0.9028\n",
            "Epoch 207/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1889 - acc: 0.9276 - val_loss: 0.2732 - val_acc: 0.8974\n",
            "Epoch 208/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1907 - acc: 0.9274 - val_loss: 0.3167 - val_acc: 0.8855\n",
            "Epoch 209/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1902 - acc: 0.9246 - val_loss: 0.2783 - val_acc: 0.8995\n",
            "Epoch 210/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1891 - acc: 0.9263 - val_loss: 0.2990 - val_acc: 0.8873\n",
            "Epoch 211/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2048 - acc: 0.9230 - val_loss: 0.2964 - val_acc: 0.8923\n",
            "Epoch 212/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1898 - acc: 0.9284 - val_loss: 0.2772 - val_acc: 0.8974\n",
            "Epoch 213/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1886 - acc: 0.9256 - val_loss: 0.2692 - val_acc: 0.9032\n",
            "Epoch 214/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1826 - acc: 0.9310 - val_loss: 0.2871 - val_acc: 0.8929\n",
            "Epoch 215/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1861 - acc: 0.9302 - val_loss: 0.2579 - val_acc: 0.9032\n",
            "Epoch 216/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1765 - acc: 0.9301 - val_loss: 0.2624 - val_acc: 0.9042\n",
            "Epoch 217/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1798 - acc: 0.9337 - val_loss: 0.3499 - val_acc: 0.8743\n",
            "Epoch 218/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1861 - acc: 0.9287 - val_loss: 0.2640 - val_acc: 0.9067\n",
            "Epoch 219/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1886 - acc: 0.9284 - val_loss: 0.2792 - val_acc: 0.8948\n",
            "Epoch 220/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1855 - acc: 0.9289 - val_loss: 0.2615 - val_acc: 0.9020\n",
            "Epoch 221/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2057 - acc: 0.9225 - val_loss: 0.2584 - val_acc: 0.9038\n",
            "Epoch 222/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1768 - acc: 0.9306 - val_loss: 0.2592 - val_acc: 0.9078\n",
            "Epoch 223/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1853 - acc: 0.9272 - val_loss: 0.2681 - val_acc: 0.9036\n",
            "Epoch 224/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1792 - acc: 0.9300 - val_loss: 0.2746 - val_acc: 0.9030\n",
            "Epoch 225/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1789 - acc: 0.9322 - val_loss: 0.2452 - val_acc: 0.9137\n",
            "Epoch 226/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1770 - acc: 0.9349 - val_loss: 0.2570 - val_acc: 0.9049\n",
            "Epoch 227/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1789 - acc: 0.9332 - val_loss: 0.3142 - val_acc: 0.8809\n",
            "Epoch 228/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1849 - acc: 0.9300 - val_loss: 0.2418 - val_acc: 0.9160\n",
            "Epoch 229/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1774 - acc: 0.9327 - val_loss: 0.2869 - val_acc: 0.8859\n",
            "Epoch 230/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1667 - acc: 0.9358 - val_loss: 0.3188 - val_acc: 0.8830\n",
            "Epoch 231/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1762 - acc: 0.9362 - val_loss: 0.2660 - val_acc: 0.9030\n",
            "Epoch 232/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1991 - acc: 0.9233 - val_loss: 0.2819 - val_acc: 0.8976\n",
            "Epoch 233/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1754 - acc: 0.9351 - val_loss: 0.2695 - val_acc: 0.9009\n",
            "Epoch 234/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.2021 - acc: 0.9214 - val_loss: 0.2699 - val_acc: 0.9016\n",
            "Epoch 235/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1731 - acc: 0.9358 - val_loss: 0.2359 - val_acc: 0.9189\n",
            "Epoch 236/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1713 - acc: 0.9312 - val_loss: 0.2823 - val_acc: 0.8970\n",
            "Epoch 237/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1751 - acc: 0.9318 - val_loss: 0.2491 - val_acc: 0.9152\n",
            "Epoch 238/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1696 - acc: 0.9362 - val_loss: 0.2878 - val_acc: 0.8954\n",
            "Epoch 239/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1824 - acc: 0.9313 - val_loss: 0.2492 - val_acc: 0.9137\n",
            "Epoch 240/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1724 - acc: 0.9358 - val_loss: 0.2741 - val_acc: 0.9034\n",
            "Epoch 241/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1795 - acc: 0.9343 - val_loss: 0.2447 - val_acc: 0.9119\n",
            "Epoch 242/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1658 - acc: 0.9354 - val_loss: 0.2434 - val_acc: 0.9179\n",
            "Epoch 243/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1746 - acc: 0.9368 - val_loss: 0.2395 - val_acc: 0.9187\n",
            "Epoch 244/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1639 - acc: 0.9410 - val_loss: 0.2501 - val_acc: 0.9129\n",
            "Epoch 245/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1756 - acc: 0.9354 - val_loss: 0.2821 - val_acc: 0.8941\n",
            "Epoch 246/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1799 - acc: 0.9307 - val_loss: 0.2510 - val_acc: 0.9135\n",
            "Epoch 247/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1624 - acc: 0.9400 - val_loss: 0.2709 - val_acc: 0.9043\n",
            "Epoch 248/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1872 - acc: 0.9291 - val_loss: 0.2500 - val_acc: 0.9152\n",
            "Epoch 249/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1612 - acc: 0.9393 - val_loss: 0.2774 - val_acc: 0.8964\n",
            "Epoch 250/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1703 - acc: 0.9346 - val_loss: 0.2758 - val_acc: 0.9061\n",
            "Epoch 251/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1596 - acc: 0.9423 - val_loss: 0.2391 - val_acc: 0.9205\n",
            "Epoch 252/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1626 - acc: 0.9385 - val_loss: 0.2302 - val_acc: 0.9216\n",
            "Epoch 253/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1521 - acc: 0.9443 - val_loss: 0.2449 - val_acc: 0.9106\n",
            "Epoch 254/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1628 - acc: 0.9395 - val_loss: 0.2297 - val_acc: 0.9193\n",
            "Epoch 255/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1563 - acc: 0.9421 - val_loss: 0.2467 - val_acc: 0.9144\n",
            "Epoch 256/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1523 - acc: 0.9442 - val_loss: 0.2548 - val_acc: 0.9075\n",
            "Epoch 257/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1583 - acc: 0.9398 - val_loss: 0.2467 - val_acc: 0.9160\n",
            "Epoch 258/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1581 - acc: 0.9412 - val_loss: 0.2618 - val_acc: 0.9086\n",
            "Epoch 259/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1658 - acc: 0.9379 - val_loss: 0.2506 - val_acc: 0.9137\n",
            "Epoch 260/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1615 - acc: 0.9380 - val_loss: 0.2391 - val_acc: 0.9152\n",
            "Epoch 261/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1610 - acc: 0.9398 - val_loss: 0.2236 - val_acc: 0.9177\n",
            "Epoch 262/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1638 - acc: 0.9363 - val_loss: 0.2270 - val_acc: 0.9241\n",
            "Epoch 263/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1611 - acc: 0.9412 - val_loss: 0.2381 - val_acc: 0.9152\n",
            "Epoch 264/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1498 - acc: 0.9426 - val_loss: 0.2383 - val_acc: 0.9127\n",
            "Epoch 265/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1652 - acc: 0.9385 - val_loss: 0.2173 - val_acc: 0.9267\n",
            "Epoch 266/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1534 - acc: 0.9454 - val_loss: 0.2649 - val_acc: 0.9063\n",
            "Epoch 267/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1510 - acc: 0.9462 - val_loss: 0.2395 - val_acc: 0.9179\n",
            "Epoch 268/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1588 - acc: 0.9431 - val_loss: 0.2343 - val_acc: 0.9185\n",
            "Epoch 269/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1553 - acc: 0.9415 - val_loss: 0.2657 - val_acc: 0.9047\n",
            "Epoch 270/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1662 - acc: 0.9378 - val_loss: 0.2596 - val_acc: 0.9026\n",
            "Epoch 271/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1499 - acc: 0.9410 - val_loss: 0.2603 - val_acc: 0.9090\n",
            "Epoch 272/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1515 - acc: 0.9439 - val_loss: 0.2340 - val_acc: 0.9187\n",
            "Epoch 273/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1535 - acc: 0.9421 - val_loss: 0.2882 - val_acc: 0.8977\n",
            "Epoch 274/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1654 - acc: 0.9373 - val_loss: 0.2319 - val_acc: 0.9166\n",
            "Epoch 275/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1481 - acc: 0.9442 - val_loss: 0.2458 - val_acc: 0.9162\n",
            "Epoch 276/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1444 - acc: 0.9484 - val_loss: 0.2219 - val_acc: 0.9224\n",
            "Epoch 277/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1726 - acc: 0.9374 - val_loss: 0.2390 - val_acc: 0.9137\n",
            "Epoch 278/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1430 - acc: 0.9471 - val_loss: 0.2249 - val_acc: 0.9234\n",
            "Epoch 279/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1425 - acc: 0.9486 - val_loss: 0.2383 - val_acc: 0.9154\n",
            "Epoch 280/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1345 - acc: 0.9503 - val_loss: 0.2507 - val_acc: 0.9121\n",
            "Epoch 281/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1550 - acc: 0.9439 - val_loss: 0.2585 - val_acc: 0.9057\n",
            "Epoch 282/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1466 - acc: 0.9453 - val_loss: 0.2478 - val_acc: 0.9106\n",
            "Epoch 283/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1672 - acc: 0.9370 - val_loss: 0.2429 - val_acc: 0.9158\n",
            "Epoch 284/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1454 - acc: 0.9482 - val_loss: 0.2542 - val_acc: 0.9179\n",
            "Epoch 285/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1669 - acc: 0.9356 - val_loss: 0.2612 - val_acc: 0.9146\n",
            "Epoch 286/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1444 - acc: 0.9466 - val_loss: 0.3066 - val_acc: 0.8976\n",
            "Epoch 287/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1389 - acc: 0.9476 - val_loss: 0.2278 - val_acc: 0.9197\n",
            "Epoch 288/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1313 - acc: 0.9492 - val_loss: 0.2140 - val_acc: 0.9232\n",
            "Epoch 289/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1383 - acc: 0.9507 - val_loss: 0.2474 - val_acc: 0.9142\n",
            "Epoch 290/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1314 - acc: 0.9513 - val_loss: 0.2325 - val_acc: 0.9224\n",
            "Epoch 291/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1406 - acc: 0.9488 - val_loss: 0.2513 - val_acc: 0.9121\n",
            "Epoch 292/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1496 - acc: 0.9470 - val_loss: 0.2329 - val_acc: 0.9195\n",
            "Epoch 293/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1437 - acc: 0.9479 - val_loss: 0.2212 - val_acc: 0.9236\n",
            "Epoch 294/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1317 - acc: 0.9536 - val_loss: 0.2435 - val_acc: 0.9150\n",
            "Epoch 295/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1368 - acc: 0.9513 - val_loss: 0.2521 - val_acc: 0.9071\n",
            "Epoch 296/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1339 - acc: 0.9490 - val_loss: 0.2229 - val_acc: 0.9205\n",
            "Epoch 297/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1418 - acc: 0.9487 - val_loss: 0.2412 - val_acc: 0.9177\n",
            "Epoch 298/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1488 - acc: 0.9469 - val_loss: 0.2152 - val_acc: 0.9278\n",
            "Epoch 299/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1622 - acc: 0.9388 - val_loss: 0.2215 - val_acc: 0.9274\n",
            "Epoch 300/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1415 - acc: 0.9467 - val_loss: 0.2062 - val_acc: 0.9309\n",
            "Epoch 301/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1348 - acc: 0.9518 - val_loss: 0.2540 - val_acc: 0.9197\n",
            "Epoch 302/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1376 - acc: 0.9488 - val_loss: 0.2322 - val_acc: 0.9193\n",
            "Epoch 303/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1370 - acc: 0.9487 - val_loss: 0.2422 - val_acc: 0.9189\n",
            "Epoch 304/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1400 - acc: 0.9470 - val_loss: 0.2290 - val_acc: 0.9201\n",
            "Epoch 305/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1388 - acc: 0.9491 - val_loss: 0.2541 - val_acc: 0.9168\n",
            "Epoch 306/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1265 - acc: 0.9531 - val_loss: 0.2243 - val_acc: 0.9270\n",
            "Epoch 307/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1403 - acc: 0.9472 - val_loss: 0.2789 - val_acc: 0.9040\n",
            "Epoch 308/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1393 - acc: 0.9494 - val_loss: 0.2235 - val_acc: 0.9228\n",
            "Epoch 309/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1314 - acc: 0.9483 - val_loss: 0.2363 - val_acc: 0.9232\n",
            "Epoch 310/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1373 - acc: 0.9530 - val_loss: 0.2125 - val_acc: 0.9327\n",
            "Epoch 311/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1296 - acc: 0.9535 - val_loss: 0.2160 - val_acc: 0.9267\n",
            "Epoch 312/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1293 - acc: 0.9541 - val_loss: 0.2589 - val_acc: 0.9043\n",
            "Epoch 313/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1413 - acc: 0.9490 - val_loss: 0.2442 - val_acc: 0.9162\n",
            "Epoch 314/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1323 - acc: 0.9530 - val_loss: 0.2271 - val_acc: 0.9251\n",
            "Epoch 315/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1261 - acc: 0.9527 - val_loss: 0.2106 - val_acc: 0.9303\n",
            "Epoch 316/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1323 - acc: 0.9515 - val_loss: 0.2122 - val_acc: 0.9325\n",
            "Epoch 317/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1402 - acc: 0.9498 - val_loss: 0.2079 - val_acc: 0.9302\n",
            "Epoch 318/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1244 - acc: 0.9554 - val_loss: 0.2281 - val_acc: 0.9222\n",
            "Epoch 319/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1313 - acc: 0.9515 - val_loss: 0.2196 - val_acc: 0.9249\n",
            "Epoch 320/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1242 - acc: 0.9550 - val_loss: 0.2216 - val_acc: 0.9261\n",
            "Epoch 321/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1295 - acc: 0.9531 - val_loss: 0.2228 - val_acc: 0.9245\n",
            "Epoch 322/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1332 - acc: 0.9528 - val_loss: 0.2717 - val_acc: 0.9144\n",
            "Epoch 323/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1656 - acc: 0.9410 - val_loss: 0.2323 - val_acc: 0.9313\n",
            "Epoch 324/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1249 - acc: 0.9558 - val_loss: 0.2425 - val_acc: 0.9123\n",
            "Epoch 325/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1299 - acc: 0.9531 - val_loss: 0.2383 - val_acc: 0.9212\n",
            "Epoch 326/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1379 - acc: 0.9521 - val_loss: 0.2033 - val_acc: 0.9311\n",
            "Epoch 327/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1208 - acc: 0.9570 - val_loss: 0.2098 - val_acc: 0.9333\n",
            "Epoch 328/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1214 - acc: 0.9549 - val_loss: 0.1942 - val_acc: 0.9352\n",
            "Epoch 329/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1259 - acc: 0.9549 - val_loss: 0.2379 - val_acc: 0.9195\n",
            "Epoch 330/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1338 - acc: 0.9519 - val_loss: 0.2084 - val_acc: 0.9315\n",
            "Epoch 331/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1575 - acc: 0.9414 - val_loss: 0.2035 - val_acc: 0.9346\n",
            "Epoch 332/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1181 - acc: 0.9609 - val_loss: 0.2993 - val_acc: 0.9026\n",
            "Epoch 333/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1358 - acc: 0.9501 - val_loss: 0.2023 - val_acc: 0.9373\n",
            "Epoch 334/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1220 - acc: 0.9554 - val_loss: 0.2623 - val_acc: 0.9133\n",
            "Epoch 335/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1778 - acc: 0.9413 - val_loss: 0.2059 - val_acc: 0.9290\n",
            "Epoch 336/550\n",
            "688/688 [==============================] - 2s 2ms/step - loss: 0.1208 - acc: 0.9559 - val_loss: 0.2077 - val_acc: 0.9302\n",
            "Epoch 337/550\n",
            "688/688 [==============================] - 2s 2ms/step - loss: 0.1192 - acc: 0.9576 - val_loss: 0.2333 - val_acc: 0.9251\n",
            "Epoch 338/550\n",
            "688/688 [==============================] - 2s 2ms/step - loss: 0.1344 - acc: 0.9502 - val_loss: 0.2087 - val_acc: 0.9230\n",
            "Epoch 339/550\n",
            "688/688 [==============================] - 2s 2ms/step - loss: 0.1224 - acc: 0.9559 - val_loss: 0.2176 - val_acc: 0.9237\n",
            "Epoch 340/550\n",
            "688/688 [==============================] - 2s 3ms/step - loss: 0.1313 - acc: 0.9529 - val_loss: 0.2566 - val_acc: 0.9189\n",
            "Epoch 341/550\n",
            "688/688 [==============================] - 2s 2ms/step - loss: 0.1248 - acc: 0.9524 - val_loss: 0.2358 - val_acc: 0.9222\n",
            "Epoch 342/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1146 - acc: 0.9600 - val_loss: 0.2122 - val_acc: 0.9280\n",
            "Epoch 343/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1209 - acc: 0.9589 - val_loss: 0.2009 - val_acc: 0.9385\n",
            "Epoch 344/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1416 - acc: 0.9499 - val_loss: 0.2047 - val_acc: 0.9350\n",
            "Epoch 345/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1360 - acc: 0.9494 - val_loss: 0.2020 - val_acc: 0.9369\n",
            "Epoch 346/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1165 - acc: 0.9580 - val_loss: 0.1998 - val_acc: 0.9315\n",
            "Epoch 347/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1207 - acc: 0.9567 - val_loss: 0.2065 - val_acc: 0.9383\n",
            "Epoch 348/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1184 - acc: 0.9587 - val_loss: 0.2269 - val_acc: 0.9251\n",
            "Epoch 349/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1209 - acc: 0.9563 - val_loss: 0.2060 - val_acc: 0.9315\n",
            "Epoch 350/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1139 - acc: 0.9583 - val_loss: 0.2112 - val_acc: 0.9247\n",
            "Epoch 351/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1210 - acc: 0.9589 - val_loss: 0.2179 - val_acc: 0.9259\n",
            "Epoch 352/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1272 - acc: 0.9554 - val_loss: 0.2416 - val_acc: 0.9305\n",
            "Epoch 353/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1109 - acc: 0.9605 - val_loss: 0.2107 - val_acc: 0.9381\n",
            "Epoch 354/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1184 - acc: 0.9584 - val_loss: 0.2085 - val_acc: 0.9313\n",
            "Epoch 355/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1118 - acc: 0.9618 - val_loss: 0.2720 - val_acc: 0.9107\n",
            "Epoch 356/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1256 - acc: 0.9544 - val_loss: 0.2193 - val_acc: 0.9282\n",
            "Epoch 357/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1139 - acc: 0.9600 - val_loss: 0.2067 - val_acc: 0.9315\n",
            "Epoch 358/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1239 - acc: 0.9550 - val_loss: 0.2232 - val_acc: 0.9296\n",
            "Epoch 359/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1166 - acc: 0.9590 - val_loss: 0.2338 - val_acc: 0.9197\n",
            "Epoch 360/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1114 - acc: 0.9612 - val_loss: 0.2235 - val_acc: 0.9284\n",
            "Epoch 361/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1119 - acc: 0.9604 - val_loss: 0.2491 - val_acc: 0.9172\n",
            "Epoch 362/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1150 - acc: 0.9589 - val_loss: 0.2262 - val_acc: 0.9179\n",
            "Epoch 363/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1362 - acc: 0.9486 - val_loss: 0.2092 - val_acc: 0.9367\n",
            "Epoch 364/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1058 - acc: 0.9627 - val_loss: 0.2220 - val_acc: 0.9340\n",
            "Epoch 365/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1150 - acc: 0.9595 - val_loss: 0.1911 - val_acc: 0.9416\n",
            "Epoch 366/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1076 - acc: 0.9612 - val_loss: 0.2167 - val_acc: 0.9284\n",
            "Epoch 367/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1117 - acc: 0.9600 - val_loss: 0.1979 - val_acc: 0.9338\n",
            "Epoch 368/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1036 - acc: 0.9648 - val_loss: 0.1926 - val_acc: 0.9385\n",
            "Epoch 369/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1127 - acc: 0.9569 - val_loss: 0.1831 - val_acc: 0.9466\n",
            "Epoch 370/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1295 - acc: 0.9529 - val_loss: 0.2422 - val_acc: 0.9177\n",
            "Epoch 371/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1103 - acc: 0.9629 - val_loss: 0.2190 - val_acc: 0.9239\n",
            "Epoch 372/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1138 - acc: 0.9600 - val_loss: 0.2022 - val_acc: 0.9356\n",
            "Epoch 373/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1119 - acc: 0.9608 - val_loss: 0.2301 - val_acc: 0.9276\n",
            "Epoch 374/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1115 - acc: 0.9612 - val_loss: 0.2046 - val_acc: 0.9364\n",
            "Epoch 375/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1528 - acc: 0.9509 - val_loss: 0.2038 - val_acc: 0.9400\n",
            "Epoch 376/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0965 - acc: 0.9673 - val_loss: 0.2394 - val_acc: 0.9170\n",
            "Epoch 377/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1041 - acc: 0.9645 - val_loss: 0.1873 - val_acc: 0.9366\n",
            "Epoch 378/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1065 - acc: 0.9643 - val_loss: 0.2136 - val_acc: 0.9214\n",
            "Epoch 379/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1049 - acc: 0.9620 - val_loss: 0.2062 - val_acc: 0.9362\n",
            "Epoch 380/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1060 - acc: 0.9638 - val_loss: 0.2135 - val_acc: 0.9327\n",
            "Epoch 381/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1111 - acc: 0.9615 - val_loss: 0.2141 - val_acc: 0.9315\n",
            "Epoch 382/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0965 - acc: 0.9655 - val_loss: 0.1925 - val_acc: 0.9430\n",
            "Epoch 383/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1191 - acc: 0.9577 - val_loss: 0.2020 - val_acc: 0.9327\n",
            "Epoch 384/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1063 - acc: 0.9643 - val_loss: 0.1879 - val_acc: 0.9366\n",
            "Epoch 385/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1026 - acc: 0.9637 - val_loss: 0.2113 - val_acc: 0.9302\n",
            "Epoch 386/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1700 - acc: 0.9452 - val_loss: 0.2071 - val_acc: 0.9356\n",
            "Epoch 387/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1043 - acc: 0.9634 - val_loss: 0.2069 - val_acc: 0.9311\n",
            "Epoch 388/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1016 - acc: 0.9654 - val_loss: 0.1914 - val_acc: 0.9381\n",
            "Epoch 389/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1012 - acc: 0.9651 - val_loss: 0.2171 - val_acc: 0.9342\n",
            "Epoch 390/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1101 - acc: 0.9606 - val_loss: 0.1961 - val_acc: 0.9408\n",
            "Epoch 391/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1051 - acc: 0.9638 - val_loss: 0.2159 - val_acc: 0.9265\n",
            "Epoch 392/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0991 - acc: 0.9648 - val_loss: 0.2352 - val_acc: 0.9334\n",
            "Epoch 393/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0990 - acc: 0.9661 - val_loss: 0.2396 - val_acc: 0.9338\n",
            "Epoch 394/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1049 - acc: 0.9637 - val_loss: 0.2109 - val_acc: 0.9325\n",
            "Epoch 395/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1086 - acc: 0.9608 - val_loss: 0.2085 - val_acc: 0.9338\n",
            "Epoch 396/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1099 - acc: 0.9609 - val_loss: 0.1981 - val_acc: 0.9364\n",
            "Epoch 397/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1023 - acc: 0.9650 - val_loss: 0.2212 - val_acc: 0.9317\n",
            "Epoch 398/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1140 - acc: 0.9575 - val_loss: 0.2471 - val_acc: 0.9255\n",
            "Epoch 399/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1200 - acc: 0.9579 - val_loss: 0.1802 - val_acc: 0.9410\n",
            "Epoch 400/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1038 - acc: 0.9620 - val_loss: 0.1917 - val_acc: 0.9371\n",
            "Epoch 401/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0947 - acc: 0.9669 - val_loss: 0.2011 - val_acc: 0.9344\n",
            "Epoch 402/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0988 - acc: 0.9637 - val_loss: 0.1952 - val_acc: 0.9385\n",
            "Epoch 403/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1221 - acc: 0.9615 - val_loss: 0.1922 - val_acc: 0.9424\n",
            "Epoch 404/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0945 - acc: 0.9671 - val_loss: 0.2188 - val_acc: 0.9309\n",
            "Epoch 405/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1030 - acc: 0.9630 - val_loss: 0.1944 - val_acc: 0.9416\n",
            "Epoch 406/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0932 - acc: 0.9685 - val_loss: 0.2160 - val_acc: 0.9354\n",
            "Epoch 407/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1033 - acc: 0.9611 - val_loss: 0.2013 - val_acc: 0.9387\n",
            "Epoch 408/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0936 - acc: 0.9668 - val_loss: 0.2082 - val_acc: 0.9315\n",
            "Epoch 409/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1074 - acc: 0.9629 - val_loss: 0.1814 - val_acc: 0.9430\n",
            "Epoch 410/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0984 - acc: 0.9662 - val_loss: 0.2288 - val_acc: 0.9313\n",
            "Epoch 411/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1026 - acc: 0.9622 - val_loss: 0.1914 - val_acc: 0.9399\n",
            "Epoch 412/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0941 - acc: 0.9675 - val_loss: 0.1856 - val_acc: 0.9385\n",
            "Epoch 413/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0932 - acc: 0.9683 - val_loss: 0.2091 - val_acc: 0.9309\n",
            "Epoch 414/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1004 - acc: 0.9652 - val_loss: 0.2093 - val_acc: 0.9317\n",
            "Epoch 415/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1103 - acc: 0.9596 - val_loss: 0.2160 - val_acc: 0.9325\n",
            "Epoch 416/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0973 - acc: 0.9678 - val_loss: 0.1785 - val_acc: 0.9470\n",
            "Epoch 417/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0981 - acc: 0.9655 - val_loss: 0.1881 - val_acc: 0.9408\n",
            "Epoch 418/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0945 - acc: 0.9673 - val_loss: 0.1993 - val_acc: 0.9352\n",
            "Epoch 419/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0905 - acc: 0.9664 - val_loss: 0.1884 - val_acc: 0.9404\n",
            "Epoch 420/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1014 - acc: 0.9658 - val_loss: 0.1682 - val_acc: 0.9490\n",
            "Epoch 421/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0942 - acc: 0.9688 - val_loss: 0.1775 - val_acc: 0.9453\n",
            "Epoch 422/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0904 - acc: 0.9690 - val_loss: 0.1962 - val_acc: 0.9360\n",
            "Epoch 423/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1018 - acc: 0.9641 - val_loss: 0.1758 - val_acc: 0.9453\n",
            "Epoch 424/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0905 - acc: 0.9693 - val_loss: 0.2031 - val_acc: 0.9424\n",
            "Epoch 425/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0920 - acc: 0.9688 - val_loss: 0.2004 - val_acc: 0.9430\n",
            "Epoch 426/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0948 - acc: 0.9684 - val_loss: 0.1805 - val_acc: 0.9455\n",
            "Epoch 427/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0937 - acc: 0.9674 - val_loss: 0.2163 - val_acc: 0.9358\n",
            "Epoch 428/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1058 - acc: 0.9631 - val_loss: 0.1889 - val_acc: 0.9389\n",
            "Epoch 429/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0952 - acc: 0.9682 - val_loss: 0.1980 - val_acc: 0.9387\n",
            "Epoch 430/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0945 - acc: 0.9680 - val_loss: 0.2221 - val_acc: 0.9290\n",
            "Epoch 431/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0996 - acc: 0.9661 - val_loss: 0.1570 - val_acc: 0.9538\n",
            "Epoch 432/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0830 - acc: 0.9730 - val_loss: 0.1683 - val_acc: 0.9501\n",
            "Epoch 433/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1376 - acc: 0.9573 - val_loss: 0.1899 - val_acc: 0.9484\n",
            "Epoch 434/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1068 - acc: 0.9641 - val_loss: 0.1948 - val_acc: 0.9435\n",
            "Epoch 435/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0918 - acc: 0.9678 - val_loss: 0.1817 - val_acc: 0.9470\n",
            "Epoch 436/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0913 - acc: 0.9677 - val_loss: 0.2517 - val_acc: 0.9255\n",
            "Epoch 437/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1200 - acc: 0.9598 - val_loss: 0.1978 - val_acc: 0.9375\n",
            "Epoch 438/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0993 - acc: 0.9638 - val_loss: 0.2078 - val_acc: 0.9360\n",
            "Epoch 439/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1004 - acc: 0.9642 - val_loss: 0.1736 - val_acc: 0.9486\n",
            "Epoch 440/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0991 - acc: 0.9662 - val_loss: 0.1908 - val_acc: 0.9404\n",
            "Epoch 441/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0907 - acc: 0.9679 - val_loss: 0.1694 - val_acc: 0.9480\n",
            "Epoch 442/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0921 - acc: 0.9690 - val_loss: 0.1787 - val_acc: 0.9505\n",
            "Epoch 443/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0980 - acc: 0.9682 - val_loss: 0.1843 - val_acc: 0.9488\n",
            "Epoch 444/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0916 - acc: 0.9684 - val_loss: 0.1870 - val_acc: 0.9406\n",
            "Epoch 445/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0995 - acc: 0.9648 - val_loss: 0.1896 - val_acc: 0.9486\n",
            "Epoch 446/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1361 - acc: 0.9532 - val_loss: 0.1668 - val_acc: 0.9482\n",
            "Epoch 447/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0804 - acc: 0.9715 - val_loss: 0.2116 - val_acc: 0.9327\n",
            "Epoch 448/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0844 - acc: 0.9726 - val_loss: 0.1631 - val_acc: 0.9497\n",
            "Epoch 449/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0847 - acc: 0.9725 - val_loss: 0.1927 - val_acc: 0.9362\n",
            "Epoch 450/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0851 - acc: 0.9699 - val_loss: 0.1946 - val_acc: 0.9463\n",
            "Epoch 451/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0936 - acc: 0.9696 - val_loss: 0.1577 - val_acc: 0.9517\n",
            "Epoch 452/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0850 - acc: 0.9691 - val_loss: 0.2036 - val_acc: 0.9329\n",
            "Epoch 453/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0848 - acc: 0.9691 - val_loss: 0.1881 - val_acc: 0.9468\n",
            "Epoch 454/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0927 - acc: 0.9662 - val_loss: 0.2060 - val_acc: 0.9406\n",
            "Epoch 455/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0876 - acc: 0.9688 - val_loss: 0.1779 - val_acc: 0.9490\n",
            "Epoch 456/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0786 - acc: 0.9726 - val_loss: 0.1831 - val_acc: 0.9472\n",
            "Epoch 457/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0928 - acc: 0.9699 - val_loss: 0.1981 - val_acc: 0.9416\n",
            "Epoch 458/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0951 - acc: 0.9676 - val_loss: 0.2076 - val_acc: 0.9348\n",
            "Epoch 459/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0902 - acc: 0.9680 - val_loss: 0.2330 - val_acc: 0.9270\n",
            "Epoch 460/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0953 - acc: 0.9660 - val_loss: 0.1788 - val_acc: 0.9439\n",
            "Epoch 461/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0773 - acc: 0.9728 - val_loss: 0.2126 - val_acc: 0.9288\n",
            "Epoch 462/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0904 - acc: 0.9685 - val_loss: 0.1836 - val_acc: 0.9418\n",
            "Epoch 463/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1764 - acc: 0.9451 - val_loss: 0.1851 - val_acc: 0.9496\n",
            "Epoch 464/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0862 - acc: 0.9722 - val_loss: 0.1985 - val_acc: 0.9433\n",
            "Epoch 465/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0882 - acc: 0.9683 - val_loss: 0.1747 - val_acc: 0.9497\n",
            "Epoch 466/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0772 - acc: 0.9741 - val_loss: 0.1803 - val_acc: 0.9459\n",
            "Epoch 467/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0856 - acc: 0.9710 - val_loss: 0.1801 - val_acc: 0.9474\n",
            "Epoch 468/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0930 - acc: 0.9667 - val_loss: 0.1854 - val_acc: 0.9480\n",
            "Epoch 469/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0961 - acc: 0.9666 - val_loss: 0.1660 - val_acc: 0.9544\n",
            "Epoch 470/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1085 - acc: 0.9607 - val_loss: 0.1608 - val_acc: 0.9538\n",
            "Epoch 471/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0903 - acc: 0.9692 - val_loss: 0.1608 - val_acc: 0.9560\n",
            "Epoch 472/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0815 - acc: 0.9732 - val_loss: 0.2024 - val_acc: 0.9414\n",
            "Epoch 473/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0942 - acc: 0.9692 - val_loss: 0.1736 - val_acc: 0.9497\n",
            "Epoch 474/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0818 - acc: 0.9719 - val_loss: 0.2884 - val_acc: 0.9045\n",
            "Epoch 475/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0893 - acc: 0.9666 - val_loss: 0.1745 - val_acc: 0.9441\n",
            "Epoch 476/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0978 - acc: 0.9665 - val_loss: 0.1638 - val_acc: 0.9521\n",
            "Epoch 477/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0854 - acc: 0.9721 - val_loss: 0.1955 - val_acc: 0.9366\n",
            "Epoch 478/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0965 - acc: 0.9665 - val_loss: 0.1712 - val_acc: 0.9523\n",
            "Epoch 479/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0856 - acc: 0.9711 - val_loss: 0.2150 - val_acc: 0.9400\n",
            "Epoch 480/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0810 - acc: 0.9716 - val_loss: 0.1547 - val_acc: 0.9546\n",
            "Epoch 481/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0827 - acc: 0.9725 - val_loss: 0.1597 - val_acc: 0.9544\n",
            "Epoch 482/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0811 - acc: 0.9730 - val_loss: 0.1675 - val_acc: 0.9532\n",
            "Epoch 483/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0810 - acc: 0.9737 - val_loss: 0.1929 - val_acc: 0.9395\n",
            "Epoch 484/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0790 - acc: 0.9732 - val_loss: 0.1951 - val_acc: 0.9414\n",
            "Epoch 485/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0967 - acc: 0.9687 - val_loss: 0.1600 - val_acc: 0.9542\n",
            "Epoch 486/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0813 - acc: 0.9735 - val_loss: 0.1735 - val_acc: 0.9538\n",
            "Epoch 487/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0926 - acc: 0.9701 - val_loss: 0.1652 - val_acc: 0.9499\n",
            "Epoch 488/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0850 - acc: 0.9714 - val_loss: 0.1850 - val_acc: 0.9507\n",
            "Epoch 489/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0804 - acc: 0.9746 - val_loss: 0.1700 - val_acc: 0.9494\n",
            "Epoch 490/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0871 - acc: 0.9691 - val_loss: 0.1749 - val_acc: 0.9476\n",
            "Epoch 491/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0891 - acc: 0.9708 - val_loss: 0.1508 - val_acc: 0.9560\n",
            "Epoch 492/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0829 - acc: 0.9713 - val_loss: 0.1682 - val_acc: 0.9523\n",
            "Epoch 493/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0803 - acc: 0.9724 - val_loss: 0.1897 - val_acc: 0.9424\n",
            "Epoch 494/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0842 - acc: 0.9702 - val_loss: 0.2062 - val_acc: 0.9338\n",
            "Epoch 495/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1119 - acc: 0.9596 - val_loss: 0.1613 - val_acc: 0.9567\n",
            "Epoch 496/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0731 - acc: 0.9732 - val_loss: 0.1778 - val_acc: 0.9457\n",
            "Epoch 497/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0893 - acc: 0.9694 - val_loss: 0.2152 - val_acc: 0.9503\n",
            "Epoch 498/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0948 - acc: 0.9661 - val_loss: 0.1560 - val_acc: 0.9556\n",
            "Epoch 499/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0853 - acc: 0.9722 - val_loss: 0.1745 - val_acc: 0.9521\n",
            "Epoch 500/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0787 - acc: 0.9733 - val_loss: 0.1852 - val_acc: 0.9490\n",
            "Epoch 501/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0675 - acc: 0.9785 - val_loss: 0.1796 - val_acc: 0.9435\n",
            "Epoch 502/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0893 - acc: 0.9677 - val_loss: 0.1569 - val_acc: 0.9542\n",
            "Epoch 503/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0783 - acc: 0.9748 - val_loss: 0.1474 - val_acc: 0.9589\n",
            "Epoch 504/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0947 - acc: 0.9684 - val_loss: 0.1835 - val_acc: 0.9463\n",
            "Epoch 505/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0763 - acc: 0.9726 - val_loss: 0.2398 - val_acc: 0.9236\n",
            "Epoch 506/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0892 - acc: 0.9707 - val_loss: 0.1897 - val_acc: 0.9385\n",
            "Epoch 507/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0848 - acc: 0.9721 - val_loss: 0.1959 - val_acc: 0.9406\n",
            "Epoch 508/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0877 - acc: 0.9729 - val_loss: 0.2405 - val_acc: 0.9356\n",
            "Epoch 509/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0853 - acc: 0.9706 - val_loss: 0.1551 - val_acc: 0.9593\n",
            "Epoch 510/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0794 - acc: 0.9750 - val_loss: 0.1753 - val_acc: 0.9532\n",
            "Epoch 511/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0766 - acc: 0.9745 - val_loss: 0.1857 - val_acc: 0.9463\n",
            "Epoch 512/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0822 - acc: 0.9721 - val_loss: 0.1717 - val_acc: 0.9536\n",
            "Epoch 513/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0704 - acc: 0.9769 - val_loss: 0.1850 - val_acc: 0.9387\n",
            "Epoch 514/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0724 - acc: 0.9760 - val_loss: 0.2125 - val_acc: 0.9342\n",
            "Epoch 515/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0792 - acc: 0.9742 - val_loss: 0.1919 - val_acc: 0.9410\n",
            "Epoch 516/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0816 - acc: 0.9742 - val_loss: 0.1571 - val_acc: 0.9550\n",
            "Epoch 517/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0752 - acc: 0.9750 - val_loss: 0.1785 - val_acc: 0.9447\n",
            "Epoch 518/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0787 - acc: 0.9746 - val_loss: 0.1663 - val_acc: 0.9523\n",
            "Epoch 519/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0771 - acc: 0.9731 - val_loss: 0.1723 - val_acc: 0.9509\n",
            "Epoch 520/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0730 - acc: 0.9751 - val_loss: 0.1861 - val_acc: 0.9470\n",
            "Epoch 521/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0789 - acc: 0.9731 - val_loss: 0.1665 - val_acc: 0.9521\n",
            "Epoch 522/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0810 - acc: 0.9738 - val_loss: 0.1774 - val_acc: 0.9472\n",
            "Epoch 523/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0751 - acc: 0.9744 - val_loss: 0.2418 - val_acc: 0.9360\n",
            "Epoch 524/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1411 - acc: 0.9555 - val_loss: 0.2010 - val_acc: 0.9435\n",
            "Epoch 525/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0830 - acc: 0.9740 - val_loss: 0.2046 - val_acc: 0.9505\n",
            "Epoch 526/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.1053 - acc: 0.9640 - val_loss: 0.1435 - val_acc: 0.9594\n",
            "Epoch 527/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0707 - acc: 0.9752 - val_loss: 0.1730 - val_acc: 0.9439\n",
            "Epoch 528/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0696 - acc: 0.9771 - val_loss: 0.2132 - val_acc: 0.9362\n",
            "Epoch 529/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0710 - acc: 0.9758 - val_loss: 0.1490 - val_acc: 0.9589\n",
            "Epoch 530/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0854 - acc: 0.9713 - val_loss: 0.2340 - val_acc: 0.9404\n",
            "Epoch 531/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0914 - acc: 0.9698 - val_loss: 0.1382 - val_acc: 0.9631\n",
            "Epoch 532/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0741 - acc: 0.9763 - val_loss: 0.2029 - val_acc: 0.9381\n",
            "Epoch 533/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0842 - acc: 0.9683 - val_loss: 0.2196 - val_acc: 0.9412\n",
            "Epoch 534/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0738 - acc: 0.9747 - val_loss: 0.1846 - val_acc: 0.9507\n",
            "Epoch 535/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0679 - acc: 0.9768 - val_loss: 0.1542 - val_acc: 0.9581\n",
            "Epoch 536/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0682 - acc: 0.9763 - val_loss: 0.1737 - val_acc: 0.9530\n",
            "Epoch 537/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0777 - acc: 0.9732 - val_loss: 0.1649 - val_acc: 0.9507\n",
            "Epoch 538/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0890 - acc: 0.9669 - val_loss: 0.2032 - val_acc: 0.9377\n",
            "Epoch 539/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0752 - acc: 0.9758 - val_loss: 0.1701 - val_acc: 0.9581\n",
            "Epoch 540/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0742 - acc: 0.9763 - val_loss: 0.1633 - val_acc: 0.9602\n",
            "Epoch 541/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0708 - acc: 0.9756 - val_loss: 0.1605 - val_acc: 0.9562\n",
            "Epoch 542/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0788 - acc: 0.9746 - val_loss: 0.1586 - val_acc: 0.9612\n",
            "Epoch 543/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0714 - acc: 0.9778 - val_loss: 0.2115 - val_acc: 0.9327\n",
            "Epoch 544/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0844 - acc: 0.9736 - val_loss: 0.1511 - val_acc: 0.9554\n",
            "Epoch 545/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0705 - acc: 0.9789 - val_loss: 0.1550 - val_acc: 0.9529\n",
            "Epoch 546/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0700 - acc: 0.9767 - val_loss: 0.1858 - val_acc: 0.9439\n",
            "Epoch 547/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0848 - acc: 0.9723 - val_loss: 0.1740 - val_acc: 0.9515\n",
            "Epoch 548/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0668 - acc: 0.9780 - val_loss: 0.1941 - val_acc: 0.9430\n",
            "Epoch 549/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0702 - acc: 0.9784 - val_loss: 0.1721 - val_acc: 0.9476\n",
            "Epoch 550/550\n",
            "688/688 [==============================] - 1s 2ms/step - loss: 0.0793 - acc: 0.9728 - val_loss: 0.1667 - val_acc: 0.9530\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MonAlseQeoCK"
      },
      "source": [
        "score = model.evaluate(X_test, test_categorical_labels, verbose=1)\n",
        "\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzMbCorVXRhn",
        "outputId": "857d1d39-952b-4c10-cc6a-1f58691de862"
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "predict_labels = np.zeros((6442,))\n",
        "for i in range(predictions.shape[0]):\n",
        "    idx = np.argmax(predictions[i,:])\n",
        "    predict_labels[i] = idx\n",
        "confusion_matrix(y_test, predict_labels, normalize='true')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.97328042, 0.02089947, 0.00582011],\n",
              "       [0.04968153, 0.93503185, 0.01528662],\n",
              "       [0.03257329, 0.06840391, 0.8990228 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "vG9QP_8_rRyZ",
        "outputId": "2e1449ab-c027-4321-a86f-7e4a7f17e86f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(figsize=(3, 2), dpi=80)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "\n",
        "plt.title('Feedforward NN: model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylim((0.5,1.0))\n",
        "plt.legend(['train','val'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "figure(figsize=(3, 2), dpi=80)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Feedforward NN: model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylim((0,1.0))\n",
        "plt.legend(['train','val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAC5CAYAAADOK0EVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deXgUVfaw35M9ISEJBAiQBJBVUdlEWVUUlHFh1JFxQVGUUWZcB3Vcx8H9p58y7jsDiqOO6Kgo4obigBsIAgKyG/YQCAQCSQhJzvfHrU4qSSfphu6kk9z3eerpqrtUnaruPnXuucsRVcVisVgAwupbAIvFEjpYhWCxWMqwCsFisZRhFYLFYinDKgSLxVKGVQgWi6UMqxAsFksZTUYhiMgbIjLNdZwsIp+ISK6IbKlH0Q4LEZkvIpPqW476QES2iMiVfpTPFJHxQRSp0VBvCkFE5opIkYjsd23/q0MRJgCpQBtVTavD69YJzvNVETm3UnplxehTOUvToL4thMdUNd61nVyH1+4MrFTVg4d7AhGJCqA81V0j8giq7wKe8EFOX8tZgkx9fwf1rRC8IiJJIvKCiGwUkRzHtD/KlR8uIreIyK8isldEFonI6ZXOcZuIbHKaBK8CUa68r4ArgIscy+RFJ/1oEZktIrscs/QlEUl01ZsrIs+KyNsisgd4WkTeE5FHXGW+duqHOccXich6Zz9GRGaIyFYRyROR1SJyXSW5M0XkPhH5VETygFtEJEJEHhORLBHZ6b5eLUzDfMc3BqhcjTiWxk0i8r2IHBCRpSJynIiMdu51n4i8KyLxrjrtReQdEdnhbP8RkXau/HgRmeL8DraKyE1erttDRD526m8VkedFpJkfct8vImuc72SziDwjInGu/AgRmSgiK50yW0Tkdlf+QBH5yvnedzu/gVjXMxnuKtvRSeviHF/pnO86EckEcpz060RkufPMskRkuoikVJL7ChFZ4vwHdojIU076PBH5R6WyFzplalY4qlovGzAXeNBLugBfA/8GWgDRwKPASiDSKTMJWAJ0x/yQzwcOAJ2d/EuBPcAgIBK4BjgETHNdZxrwhus4AdgKPAHEAW2Bb4APKsl8ADjLuW4cpumxyMlvBuwHfgP6O2mvAi84+7HAOCDJqX82cBA403WNTCALGOg8izjgLif9GOd5PODcz6Tani9wHrAXaO2kv1HpOfha7lIgt5bvVIGfgU4YBfwOsB6Y6jzfNs7xnU75cKf8W0Ci81zeAX4Cwp0yLwOLgTTn+U4FioErnfwUYCfwV+fZpABfAq9Ueqbja5D7ciDDed49gXXAI678B520E53vrQUw0MnrCRQA1znfVRRwKhDteibDXefq6KR1cY6vdO7nZSAeiHPS/wB0c67XAfgReMt1nvHOfQ8HIpzne4rru9oEhLnKfwH8X63/y3pWCIVArmu7HOgLFAHxrrLhzkMf4hzvxfUnct3wPa79JyrlL6JmhXCJ84AjXGl9nC8v1SXzW5XOexRQ6vwQz3HKPOWSZSNwfg3P4UO3rM6P95FKZdYCN1R6Htn4oBCc/TnAqzUphNrK+fidKnC56/j3TlpbV9ozwPvO/kDn2SW78ls6aQOcP0MhcK4rP9HJ9yiEicD3leQYjFG0HqWSSQ0Kwct9/JVyJS9AHnBhNWWfBT6u5Zn4ohDiapHpfCDHdbwcuLWaslHO7+Ns57izc42jarv3COqXx1X1HneCiFyE0XhbRKRy+XQRaQM0B2aISKkrLxKjxcG8TT6sVPe3WmRJBzaqarErzXO+DMxbu8p5VHWDiGwARmB+4J8DS4E7RGQG0B74yrm3aOAhYBTmbamYt8p/apE1zZ2mqiUisqmW+3FzE7BIRJ4LULma2O7aPwCgqpXTEpz9dGC3qu7xZKpqjtMcy8DcczQV732viOx2na8r0E9Ecl1pgqPIMVZfjYjItcC1mDdxBOa3lONkp2De3Kurqd4JWFXbNWohW1XzK8l0AXAr0AWIwSjHZiISrqolznW9yqSqRSIyBXNPszAW8hxV3VCbIKHoQ8jCWAitVDXJtcWq6lsYS6IQOKdSfjNV/bNzji0YTeym8nFlNgMZIuJWkp2dT/efz62EPHwBnOFsn2Heun2AC4EFqrrXKTcRONfZklU1CZiN+QG7qXyNCvcjIuGYP5NPqOpyTNPlqUCUCyCbgWQRSfYkiEgLIBnzzHdi3vQdXfmJTr6HLGB+pd9CoqrGqKovymAg5i1/C8YSTATupvw72YVpBnar5hSZNeTh1HX7M9p5KVPh+xaRNGAGxprKUNXmGOsZl1y1Xfcl4EwxvrdxwIs1lC0jFBXCfIw59IKItIayMQN/EJE4Nb0CLwKPiXECiojEisjJIuJ5QK8BV4nIAMchNB7oVct1Z2HMqoed86UC/wQ+UtWsmqvyOcakawksVtUDwALgNifPQyLmB74TCBOR0RglUhuvYZyLPRwr415MO9Yf7gWOBc4MULlAsADzXT8rIs2dP/tzGP/QQlUtxTRdJjnOx2YYH497EY+pQB8R+YuIxDm/h3QROc9HGRKBEmCnqh4Skb7A9Z5MNTb3M8AjInKCc/4WjiIBeAEYISITnN9NpIic4nxPYPwhV4pxKLcBKjj7qiEe89/cpaqFItIVuLNSmaeA20XkNDFO9gQROcUldybmRTUD42/6yJeHEXIKwTGHRgD5wI9iPO1LMX84zw/hVowjagbGYsjEPDBPF92/gcec/F2Y9uj7tVx3n3PdXpg38iJMk+EKH8T+CvMW+NL5AYGxFBKpqBAex7wVNwLbgNOBD3w4/6PAfzFOzi2YNuKPPtQrQ1VzMM7YlMMpJyJjRGS/P9f0QaYSjN8lGvOs12JM9lFOHpj2/C/Otsb5zHKdYxOmqTYC47DMxTz743wU43PMC2auiOwFHsYoYDf3Yiynf2P8CcuAoc71l2Mce5dgvtMdTnnPf+s6TNNlF+YPOr02gVR1Feb3/Lrz+38NoxjdZV52yjyJcaCvxTiG3byA8clNqdQUrhYp//1aLJbGhIgcjbHAOjmKs/Y6ViFYLI0PZ7zBFCBKVS/ytV7INRksFsuRISJnY5oRPTHOUp8JukIQkafFjL5TEeldQ7mrRWStiKwXkVfkyIbsWixNFlWd5fS69VVVvybu1YWF8C4wBONI84qIdMKMvhuK6Xdtg+k7tVgsdUjQFYKq/s8HLXUhMFNVsxwv/YsYr63FYqlD6nukoocMKloQmU5aFURkImaADwDh4eHtU1NTgyqcxVLfbN26tUhVo2sveWSEikLwGVWdDEz2HKelpemWLQ1ufROLxS9EZGddXCdUehk2YcaRe+hIxeHCFoulDggVhfAeMEpEUsXMaJoAvF3PMlksTY6gNxlE5CXMvP9U4DMRyVPVLmIWLZmpqjOdGYP/AL51qs3FTM6wWBoFO/MOUniohMjwMJLiIvli5Q7W79xPUXEp5/dpT9c2CbWfpA5o8CMVrQ/BEkgKD5UQExkOmLVCRKTsM6/wECu37eOEji3IzitE1Uyu2bGvkBfnrqdN8xg+WLKVvEIzbeCc49tyqKSUz1bsqPW6C+46ndbNY6rNF5GtWgdrfzY4p6LF4g+eF55nbY3Nu/P5YUMOp/VoTXbeQfYcKGL+ul08P3c9Z/Zsw1ersgkPEwoPeZvlbogKD6OopPp8Dx8v215rGQ+frshi7MCOPpcPFlYhWBoMBUUlhIXBttxCNuzcz459BxnatXxS5q/b9xEZEYYAn63IIjE2is9XZLFh1wGfzu95kx8qqd5qjooIq1EhpCXHclqP1qzOyqNHagKlCtN/KO9RH9WrHcenJdK5VTxHtWrG1G8z6dw6ntH9QmPhb9tksIQU+UXFvLd4KxedkM6+wkMs3riHUlUOFpdy09tLgn79vhlJNI+NZO7qnfTrkExcVDglpcrNw7vRPDaCHqnNyS8qZvPuAgoPlfDpiiz6pCdxcrdW7Mw7SHqLuCrnLCgqIbZkH7x0Cpz5MPQ4G1Z+AM3bQ/qJPslVV00GqxAsdc6OfYW8//NWSlU5VKwkxERw/8cr6dmuOSu27Tuic/dKSyS34BAbc8yKZD3bNedfV/bng5+3cnK3Vvy4IYdLTsogOiK8TJYX5q7n5uFdiY+OIEyEsDBhy558WiVEE00xFO6F+NYVL1RaAvuzYccKSO8P8yZD0X7IWg4pXeH3z5aXffdqWP5u+XFKN9i1xuxP2osvWIXgI1YhhCZFxaU89/U64qLCaZUQzcR3lh7R+ZLjImmfHMvgLiks27yX7zfkEBMZxpxbTiW1eQzbcgsQgbRk84ZWVWYu3cawHq1pHuOaJ/fdM9CyK3QfWUlgZ0nDKNcbfv9OeLyL2Z/wLXx0IxTsgZgkiE2C9V9VL3Cb42D/Drj0P/DKsOrLHXshJKRC70uhTc9qi1mF4CNWIdQfa3fkMX/dLrOfvZ//Lt5SozOuNu45+2h6pyeRmhjD8q37mPDGItKSY/nmtmGUlCpREeXDZvYfLEZVSYipZVJsUT78/Ab0HQsR0XBfkkm/dR188XfYvABadoG1n5n0Uc9C7zEwYyz86tOqY4Fh9DToeX612VYh+IhVCMGntFRZlZXHmh15/Ovb31i2xTcz101EmDC4Swp9MpJ48su1ADz2h+MZfUJaWQ9AZXLzi4jQIuJfOwMG/gX6XObfRedNhjn3lR8ffzEs82G8W/ezYfUs/651pNy6DuJbVZttFYKPWIUQGNRx3AG8u2gLHy7ZSpvmMWTvO8iCzN011u2U0ozkuEgWb8olJT6als2i+OuIbrRPiuW4tES+WrWDY9omkppo+tk35eRTcKiE7qm1DMbJ3w0vDoF9zuLJ42ZDqx7wWCcYdAP0vRK2LITel8Caz2DRNFj9yRE+CR+55G146+LAna8WX4Idh2AJKiWlSsGhEmIjw8k5cJBHZ6/mvcW+KdbkuEgSYyPJzMnn2pOP4o7f9aj2LQ9wWo825QfFB8mIyYef/gkyBlp2hseOMg65E66G/F3Q6WToPx6eOwkOZJfXnfq78v3vnjEbmDfrm3/05/Z948yHofPp8PxJVfO6uhalHj/HWCK/VYpV3G8cLJpa8zWi4mHMjCOXNUBYhdDEWLsjj7vfX17rWx+gbWIMA45qycX902mVEM03a3YydmBHwsOq//OTvxviWkBJMXz3NBx9Lqyebf7gedth2jmQt82UXTgFrplrlAHAT1PM58oPzR/RrQxq4o0/+FbOG9cvgmf7ec875jxIbF8xbexM010Y5poGFBUPfa+oqhBSukHGQNj0fXna7x6D7Uthyb/N8RkPQIdBhy9/gLEKoZFyqKSUV+f9RngYzF29k8Wb9tTo8LtlRDciwsPYmptPp5R4rh7SqTyz+CCERXLU4E7V1gfg878bJTDmXchaBnPuL2/Dr/0cMudVLF9cAEvf8n6up6tdbc8/ouJh7IfmT/zGBZC9sjzv2AshqdKyGwntyhVW5a7GP38PbY4pP77iY+OTSOkKu9eXp9+w2FgvfS83zsyC3RAeBbHJxrEJ0L4fzJoInU8LzH0GCOtDaCQUl5Sycvs+3lqwmc2788u8/27aJsawa/9BYiLCufXM7uTmH2JjzgFuG9mdtomx3k+8+HWYeQOceA2c9f+gtBR2/ALJnSCmOez+DSJiYNdqeP33Qb5LF70vM46/gj3labHJMOIBSO5o/vhR8dDzPIhyAiflrDfNjv3O3IJh98Apt1U875f3wXxnuQ1Pu36SEwD85l+qKhAPK96HGVdWrBdArA/B4hO5+UX8Y+YKPlyyrcZyD51/LGNO6lAxURV2b4DmMVCQC2HhULjPmOBHnQon32aUAcCCl+HUO2HqWbDzV0gfABe/Gbg3+QWvwH//VDV94PXw/bMV07qMgPOeg+J/wg/PQZ/LYfbtcMJV0HGwKdNpaNVztewMt66BnauN9XKSl2U7h90N7XpDj3Oq5kVUP/mIll3Np789ISGGtRAaECWlSniYkF9UzEUv/cAvW72/ia4a3Il/ffsbD59/HBf3T0ehYrs/b4d5u4ZFwszrzdt22dvQYTBIGGz42pSLSoCivCMXPH0AbP6h+vzIOLjbmQg07ZyKTYvBN8O3T1YsX9ObOhh4LIQ7NhurqDpy1kNiOkREBVwEayFYysjeV8iJD8+psczHNwxh9vLtXDmoE60Sorn9d92JCg+r6v3f8A28PsrshzmDepY4UcJ++6Zi2cNRBpd/ANMrRRTrd4XpPchZV7X81V9Cc1f8U3X8HLHJpp194jWQMcA4JHetM06+ulQGbmqyEMBYIA2culggpSsmNl0KsBe4UlVXVCoThonFONKR6Vvgz6paFGz5Qpnv1+fwv7U7eWHu+mrL3D6yB0O7pnBs+0SObZ9Ylu4Zq19G0QFjKnuUAUDpoUCLDJ2HwcVvwduuRbMT080Ygse7Gm/8rzNN2z/1eDMPwE2HwbDxWxhxv3HIQVVPf30R3vhDhdSFhfAS8LKqThORC4FpQKVfAVdjglL2xUSqfRm4Cfh/dSBfSJG1t5CZS7fyyOxVeGvNndipBdcN60KvtETCw6T6obuqphusw2DjyZ95vfdytfGnr6uOxR9xP3xxb/nxX36A5weY5gZAWZxWh+btjMfe42z7dab57Oclju6pd5j2f0cvPoD6poaxFo2FoCoEJ5z7CZSHPH8PE/q7i6q67cdemMjJRU692ZgIxI1eIagqOQeKOFhcyllPzWNvQcW3drc28Yzul84ZPduwdMtezjimTdmKPgBkr4L3r4VRz0Db40079ptHodMp8OFfDl+wu7abP3hkDPzlx4qDc3qeD4iZCwAQnQC3rS9XCBmDQMJN/3pYBCRVcmZ2P9s0U1r1qHrdsHAzMCmUOOvxit2VjZigOhVFpB/wpqp2d6UtAO5Q1a9caeOAazGKowATMvssVa3iwakclyExMbF9bm5u0O4hmOw+UMTd7//C7OVZVfLGD+nErWd2L//zF+TC5/cYT39ie/j2KYhrCR9e5/3kElbeHj8cKnedHdwPjzimu8e5VuZs2wQxifhM8UEzbbh938OXr4nR1JyK0zDLsH+DUQhfUm5VVMBbXIY6kC9glJYqr8zbwCOzV1VIT4qLpLhEuXl4Vy45MYNm0ZW+mi/uhZ+nm3H9p9xR0WT3RnXK4MyH4bO7yo8TM2DvJhgysbz//cJ/Va0XHQ83/mwsEo+n/bqFZgCSP8oAzOAcqwxCkmBbCK2BdUALVS12lljfDgyp1GSoXO9i4DpVrbUhGerdjgVFJfz4Ww5Du7bio6XbuP/jlew+UO4rjY+O4IuJJ3sfGJSXZRyBB/PgqwdN//+R8o/c8inAUG4JqMK6OcZcD0K3meXIaBQWgqpmi8hi4DKMFfAHYEtlZSAiMUCsqu4RkRTgDuDvwZQt2Ozaf5CoiDDu+u8v1S62ed2wzlwztDOJkSUw80YYdCOkdIGVM80SW8vfC4ww/cfDwlfNfnWOMRHoOjww17M0WOqiyXAtME1E7gL2AeMA3HEZgERgroiUYoLHPKWqdbg6ReAZ8uhXXucOvHR5P844pg2ybTFsmgNxPeCXD2Hxa2Zr28tMfvGVo4ZB/6vhP84IuVPvhK4j4BVnjPzYmXDUKeUKwc1xow/jziyNmaArBFVdDQz0kj7etb8DODrYsgSbouJS/v3jRj5fsaOKMnjg9z25/KQM8yYWKf/D9jy/fLYf+KcMAMZ+UPG418VmLP8Ni01bPdGLlTl2JhwqqLqMmKXJEypOxQbNvsJDXD1tIQsz91TJe+2qEzmlWysoLoL7kyH9JDNLzsPkI9CD/caV71/9pRmO7Oni8zZqLraF+TzqlMO/pqVRY+cyHCa5+UXERUWwr/AQd7y3jC9/rTp3//3eP9Gn/8lm9N4/j4W9m2s+6cRfq1cQw+6GLqeb7jrPhKN7ssun09ZG0QFnXEE1sxotIU2jcCo2Vn7Zspdzn51fJf3ec47hxE4tSE+OQ4vySHryUlg12cyFL6llFHZMkhnRN252xZWBwMygO+VvZr99v3KF4KsygPIpwBZLDViF4Cdb9uRXUQY9UhN4/eoTaZ3gTH4pOQT/e7y8QG3KAMrf3B0GwZWzYNrZ5Xkp3SqWvXQGlBYfhvQWS83YJoOPLMzczd3v/8KaHcYB2FMy6S6buOSK6+ifOxuKC2HgDWZprftaVB3PXxPHjYaT/gxprqW8ti+FFp3N8mPHXmCG9FqaLHbVZR8JtkJYlbWP+2au5PsNOYBZTryrbGZ25N+8Vzj3aRPQozLRiXDN1/CMa4Rej3Ng1cdmsFATmDhjOXysD6Geyc4r5B8frqgwz+CB4W0Y02IVYTOrUQZglEF0Ihx0zQXo/yc422lCDL7JzEMAuMhZh8AqA0uI4LNCEJEw1SOZLdMwUFUenPUrU+b/VpbWOiGap4aUMPDr0307iUcZJGZAsxQYOrE8b/DNRiH87jGrCCwhhz8WQqaIvAi8oqo7gyVQfbJ2Rx5PzlnLLNdQ4496/cCxzfYhP31WtcIfp8Mxo+Cbx+DrhyrmpR4PE+ZVrRPXIiiLcFosgcAfhTAC+DOwXEQ+B55V1R+DI1bdsjf/EAsyd3PDW4toXZzFnSnruKjZIppHlhK2ekH1FVOchTU93X/N04xFsH0J7NkYfMEtlgDjs0JwhiDf7MxJuAx4R0SygX8Cb2kD9U7uLTjE2f/8kp15hXwadTudonfAfsxWmaQOkOv6o8cmV8wXgZH/B1NHQrtewRTbYgkKfjkVnenLZwAXYf4ybwGXAKOB6kPXhij7Pn+UxO8eZj5ALetn0nGoWf3Hm0Jw68IOA+HSdyCt8ipxFkvoE1Z7EYOI3AlsAMYDj6pqT1WdrKrnAscGS8BgsX3bZpp/97DvFU6+repgIE9TodfFkNAWfu/ED+h2pvEVWCwNDH8shHbAmaq6xkteAMPgBp/dB4o4+PKZtRf0cHumsQa+edR7fkIq3LLKe57F0oDw2ULALHi6yXMgIrEikg6gqosCLVhQyMtCZ93KlFeeoiNOiPHfPw/hleYEjHkPuo2Em5ebBUY9TYNSP0YfWiwNEH8Uwrs+plVARLqKyHciskZEFopITy9lwkRksoisFJFlIvK1iHTxQzbfWDkTWfgKt+U+CIAOvhn6jIGSg+VlRj1rVg669D+QlA6tXSsDe9bl732ZsRoslkaGPwohSlULPQeqWgD4Mt3OE5ehG/AoZim1yowCBgO9VPV4YA7gRwPfB9Z9CbMrBvYUT7ehhwv/ZSL2Vsd5z8PRo+DMh6r2MFgsjQB/FII6i6YCICKpQI1D7VxxGZwxurwHpHt5+ytGucQ4PRnNgcBNUNi6yAQwrczxlVwfMUlVy7hJyoCLpkNsLeUslgaKP07Fp4HvRWS6c3wZcF8tddKB7apaDKCqKiKbgAzMaswePgKGAVlAHrAV8Lqsj5e4DLVLPsWLA7HLCAh3bj+hrYkdGNey9nNZLI0Yny0EVZ2KCbkW52zjVHV6zbV85gRM12V7TG/GHODFauSYrKppni0+Pr7mM+9YUTWG4ahn4IKXy4+v+syktQtQaHOLpYHi18AkVZ0LzPWjymagrYhEuOIyZODqrXAYC3ylqrkAIvIa8Lk/slXLL178nsdfXDH2QHIHSB4bkMtZLA0ZfwYmxYrI30TkTRH5r2erqY6qZgOeuAxQTVwGzICn00TE8y89B1juq2w1kp9TNc0GIrFYvOKPhfAKJq7CIOAJ4Ergfz7U8yUuw3OYZdiXisghjC9hgh+yVc+BnZRIBL0KXmTWuUqHcC8KwmKxAP4phF6qepyILFPVZ0RkGjCrtko+xmU4CPzJD1l8RvN2sJskwmKa027ACAj3p2PFYmla+KMQCpzPYhFppqp5ItIqGEIFjCVvIdsW0UKFC/qlEWmVgcVSI/4ohN0ikgx8AnwmIrsI5FiBYOD4CsJFGdajdS2FLRaLPwrhbFUtEZG/A5cCycDrwRErQCSmA1BMGCd0sCMLLZba8EkhiEg48Bkw3FkI5d9BlSpAHGjVmzeKz2VPh5HcEW3Xk7VYasOnRrWqlgBxItKgGuG784t5pPgStH2/2gtbLBa/mgwLgY9F5A1cC4w53YYhSX6Rma4cE2mDnFgsvuCPQjje+XR3DyoQsgqh4JBRCHFRViFYLL7gzyKrw4IpSDDILzJLnsVahWCx+IQ/gVpO9pauqr6MVqwXCh0LIdY2GSwWn/CnyfCEaz8G6I6Zb9DXe/H6x+NDsBaCxeIb/jQZKqwrLiInYuYzhCwehWB9CBaLbxx2N6KqLsDLHIVQwtNksL0MFotv+ONDON51GA6cBEQGXKIAcl6f9gzq3JJ2SbH1LYrF0iDwx4fwoWu/GFgLXBFYcQJL85hImseEtM6yWEIKf3wInYIpiMViqX/8WTHpXBFJch0ni8jZPtTzJS7DOBFZ4tp21bYak8ViCTz+OBUf8Kx56JALPOBDvVrjMqjqVFXt7dkwKyY1iAlUFktj4kh6GRTjXKwWP+IyuOucBLQmhIdEWyyNFX8UQp6IDPIciMhgTAyFmqgSlwGz4nJGDXWuBqar6iFvmSIyUUS2eLb9+/d7K2axWA4Df3oZ/ga8LyKeMMddgfMDKYyINMNEkh5QXRlVnQxM9hynpaVpIGWwWJoy/vQyfC8iR1M+GOm7Sj4Fb/gal8HDaGCFqq70VS6LxRI4/Oll6A8Uq+onqvoJUCoiJ9RUx4+4DB6uBqb4KpPFYgks/vgQXgLyXcf5VBNurRLXAteKyBrgDlxxGURklKeQiHQHegP/8UMmi8USQPzxIYQ5S6kB4DQBaq3vS1wGV7kEP+SxWCwBxh8LoUhEunoORKQb4LUnwGKxNEz8sRDuA+aLyGxAgDOAq4IilcViqRf86WWYJSJDgBGYtRTvU9UNQZPM0iRR1bKtKSEihIXV/6Lm/kx/bg38FeP4iwH+JCKoasiumGRpOJSWlpKdnU1ubm6TUwYeIiMjycjIICqq/qKT+9NkmALMB4YDt2B6D34OhlCWpsfGjRsJCwujY8eOREY2vSnrqkpOTg6bNm2iS5dqR/YHHX8UQrqqPioil6nqRyLyGfAN8PcgyWZpIpSWllJYWEjXrl2JiGi6EbZatmzJ7t27Kf+S5JcAAAm1SURBVC0trbfmg1+9DM5noYi0xCySkhJ4kSxNDU8TwQxkbbp47r8+m0z+qOM1jiJ4A/gR2AcsCopUFoulXvDZQlDVy1Q1R1Wfwiyddi/lQ5ItlkbHpEmTKCws9Lvetm3bGDp0aBAkCj6H1VBR1W9V9WPPtGaLpTFy3333eVUIxcU1/+zbtWvHvHnzgiVWUGm6HhxLSDP+tYVszMmvveBh0KFlHK9e0b/GMhMmTABg6NChhIeH065dO1JTU1m3bh3Z2dmsWrWKMWPGsHr1aoqKikhPT2fKlCmkpqaSmZlJ7969yc01k4FFhIceeogPPviAnTt3cu+99zJu3Lig3NuRUv8jISyWEOTFF828vXnz5rFkyRJat27NokWLmDVrFqtWmSVBnnzySX766SeWLVvG0KFDmTRpUrXni46OZsGCBcyePZsbb7yxViujvrAWgiUkqe0NXh+MHj2ahITy+Xdvvvkm06dPp7CwkMLCQlJSqu90GzNmDAA9evQgIiKCrKws0tLSgi6zv1gLwWLxkfj4+LL9+fPn8/TTT/PJJ5+wfPlyJk+eXKMDMiYmpmw/PDw8ZC0EqxAslmpISEhg7969XvP27NlDQkICLVu2pKioiJdeeqmOpQsOQVcIvsRlcModJyJzReRXZ7sg2LJZLDVxyy23MGLECHr37k12dnaFvJEjR9K9e3e6d+/O0KFD6d27dz1JGVgk2KOiROQr4HVVnSYiFwK3e4kkHYcJLT9WVeeLSDjQQlV31nb+tLQ03bJlS1Bkt9QNJSUlrFmzhm7duhEe3nQD89b0HERkq6oG3ekQVAvBj7gMlwI/qOp8AFUt8UUZWCyWwBLsJoOvcRmOAQ6KyMdOKLfXRaRVkGWzWCyVCBWnYgRmWvW1QB9gK/CCt4I2UIvFEjyCrRDK4jIA1BCXYRPwtapudayIN6gmWIuqTlbVNM/m7gqyWCxHRlAVgh9xGd4B+otIc+f4LGBpMGWzWCxVqYuRitcC00TkLsyU6bK4DMBMVZ2pqptE5GHgOxEpxTQZrqkD2SwWi4ugKwQ/4jJMB6YHWx6LJVhMmjSJ3NxcnnzyyfoW5bAJFaeixWIJAaxCsFi88NBDD3H99deXHe/fv58WLVowb948hgwZQt++fTnmmGN48MEH61HKwGNnO1pCkzcvhj2/BefcyZ3g0rdrLDJ27Fj69evHE088QXR0NDNmzGDYsGH07t2bOXPmEB0dTUFBAYMGDWL48OEMGOC1U6zBYS0Ei8UL6enp9OnTh5kzZwIwbdo0xo0bR0FBAePHj+e4445jwIABbNy4kSVLltSztIHDWgiW0KSWN3hdcNVVVzF16lT69evHunXrGDlyJBMmTCAlJYWff/6ZiIgILrjggsNadzFUsRaCxVIN5513HgsXLuSRRx7hsssuIyIigj179pCWlkZERASrV6/miy++qG8xA4q1ECyWaoiOjuaPf/wjzz//PL/++isA99xzD5dffjmvvfYanTt35rTTTqtnKQNL0Kc/Bxs7/bnhY6c/Gxr99GeLxdKwsArBYrGUYRWCxWIpwyoES70TCkFOQ4FQCHprexks9U5YWBgxMTFs3bqVNm3aEBkZWd8i1TmqSk5ODpGRkfUWCh6sQrCECB06dCA7O5vMzMwmaylERkaSkVF5dcG6xSoES0gQFhZGamoqbdq0QVWbnFIQkXq1DDwEXSGISFfgNSAF2AtcqaorKpU5FZgNrHYlD1TVgmDLZwktRKRe29BNnbqwEF4CXnbFZZgGeAvct1pVG0e0C4ulgRIqcRksFksIECpxGQA6i8hiJ9zbX4Isl8Vi8UKoOBUXA2mquldE0oBPRGSXqr5TuaCITAQmupJKRCSrhnPHA00xeIO978ZFnQQuCurkJqfJsA4Tp7HYicuwHRjiZSl2d707gXaqekMAZNhSF5NCQg1735bDISTiMohIWxEJc/YTgHOAn4Mpm8ViqUpddHxeC1wrImuAO3DFZRCRUU6ZPwC/iMhS4AfgC2BqHchmsVhcNPj1EGpDRCaq6uT6lqOusfdtORwavUKwWCy+U/9jJS0WS8hgFYLFYimjUSsEEekqIt+JyBpnwFPP+pbpSBGRGBH5wLmnpSLyhWfkp4i0FpFPRWStiCwXkZNd9arNa2iIyDgRURE5zzluEvddFzRqhUD5PIpuwKOYeRSNgZeB7qraC/gQeNVJ/z/gB1XtiunNeVNEIn3IazCISEfgT5jeKA+N/r7rDM9U08a2Aa0x4ecjnGMBsoAu9S1bgO/zBCDT2d8PpLryFgDDa8trKBvmBfYl0A+YC5zXFO67LrfGbCH4M4+iIXMT8KGItAQiVdU9jDsTyKgpr86kDAwTgW9VdZEnoYncd50RKnMZLIeBiNwFdAFOB2LrWZygIiLHYgawWR9AEGnMFsJmoK2IRAA48ygyMFZCg0dEbgUuAH6nqvmqmgMUi0iqq1hHYFNNeXUlbwAYipF5rYhkAgMwvpQ/0rjvu05ptApBfZxH0RBxZnxeAoxQ1VxX1gxgglOmP9Ae+MaHvJBHVV9Q1baq2lFVO2Kciteo6gs04vuuaxr1SEUR6Y7pWWiJcTCOU9Vf6lWoI8SZHr4Z2ADkOckHVfUkEWkDTAc6AUXA9ar6tVOv2ryGiIjMBZ5U1Q+a0n0Hm0atECwWi3802iaDxWLxH6sQLBZLGVYhWCyWMqxCsFgsZViFYLFYyrAKwWKxlGEVgiVoiMipIrKkvuWw+I5VCBaLpQyrEJooItJfRL4SkZ9E5GcRGS0iHUUkV0QeF5FlIrJCRIa76lzupC8TkVki0t6Vd7uI/OIs2vKDiMQ5WREi8ryTvkJETqjzm7X4Tn3Pv7Zb3W9AEibuRVvnOAUz4WcwoMDVTvoAIBtIAI7FrCfR3sm7G5jt7F+BWWcg0TlOBsKBU4Fi4CQnfQLwWX3fv92q36yF0DQZBBwFzHba+F866d0xf+BpAKr6A7AN6AMMAz5V1a1O2eeB00QkHBNY50VV3evU26OqJU65dar6o7P/PdA5mDdmOTLseghNEwFWqOqgColmeTJveJvw4uskmELXfgn2NxfSWAuhafId0KmSf6A3EIX5w17upJ0ItAOWAF8DI0WknVNlAjDHsQRmAhNEJNGpl+RYDpYGhtXWTRBV3SMiZwOPi8gTQCTGh3AzsBc41gmrFwFcqqp5wHIRuQ341Kw1w2bMYqeo6nRHUXwnIsXAAWB45etaQh87/dlShtNkWKKqSfUsiqWesE0Gi8VShrUQLBZLGdZCsFgsZViFYLFYyrAKwWKxlGEVgsViKcMqBIvFUoZVCBaLpYz/D9rs9L6ZI+0NAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 240x160 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 240x160 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}